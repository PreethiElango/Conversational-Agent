{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to see internal processing steps:\n",
      "Enter you choice : yes\n",
      "Stems : ['tell', 'me', 'some', 'car', 'which', 'are', 'in', 'my', 'budget']\n",
      "Stems : ['cheap', 'car']\n",
      "Stems : ['cheapest', 'car']\n",
      "Stems : ['afford']\n",
      "Stems : ['low', 'budget']\n",
      "Stems : ['lower', 'end', 'car']\n",
      "Stems : ['low', 'maintainc', 'car']\n",
      "Stems : ['low', 'cost', 'car']\n",
      "Stems : ['afford', 'luxuri', 'car']\n",
      "Stems : ['high', 'class', 'car']\n",
      "Stems : ['high', 'end', 'car']\n",
      "Stems : ['expens', 'car']\n",
      "Stems : ['what', 'car', 'i', 'should', 'buy']\n",
      "Stems : ['which', 'car', 'to', 'buy']\n",
      "Stems : ['trend', 'car']\n",
      "Stems : ['differ', 'compani', 'of', 'car']\n",
      "Stems : ['which', 'is', 'the', 'latest', 'car']\n",
      "Stems : ['tell', 'me', 'some', 'mileag', 'car']\n",
      "Stems : ['what', 'car', 'i', 'should', 'buy']\n",
      "Stems : ['tell', 'me', 'some', 'good', 'car', 'name']\n",
      "Stems : ['tell', 'me', 'some', 'car', 'compani', 'name']\n",
      "Stems : ['what', 'is', 'a', 'good', 'car', 'for', 'student']\n",
      "Stems : ['good', 'car', 'for', 'elder']\n",
      "Stems : ['good', 'car', 'for', 'women']\n",
      "Stems : ['sport', 'car', 'under']\n",
      "Stems : ['sexi', 'car']\n",
      "Stems : ['race', 'car']\n",
      "Stems : ['car', 'for', 'boy']\n",
      "Stems : ['how', 'much', 'mileag', 'car', 'give']\n",
      "Stems : ['mileag', 'of', 'a', 'car']\n",
      "Stems : ['mile', 'per', 'gallon']\n",
      "Stems : ['mpg']\n",
      "Stems : ['can', 'i', 'switch', 'to', 'synthet', 'oil']\n",
      "Stems : ['do', 'you', 'have', 'a', 'car', 'that', 'fit', 'my', 'need']\n",
      "Stems : ['doe', 'car', 'break']\n",
      "Stems : ['doe', 'car', 'have', 'headlight']\n",
      "Stems : ['should', 'i', 'buy', 'a', 'car']\n",
      "Stems : ['do', 'you', 'have', 'a', 'car']\n",
      "Stems : ['can', 'boy', 'drive', 'car']\n",
      "Stems : ['can', 'men', 'drive', 'car']\n",
      "Stems : ['can', 'women', 'drive', 'car']\n",
      "Stems : ['advantag', 'of', 'car']\n",
      "Stems : ['pro', 'of', 'car']\n",
      "Stems : ['use', 'of', 'car']\n",
      "Stems : ['who', 'use', 'car']\n",
      "Stems : ['disadvantag', 'of', 'car']\n",
      "Stems : ['common', 'issu', 'which', 'car']\n",
      "Stems : ['con', 'of', 'car']\n",
      "Stems : ['use', 'of', 'car']\n",
      "Stems : ['ugli', 'car']\n",
      "Stems : ['what', 'are', 'some', 'common', 'repair']\n",
      "Stems : ['how', 'car', 'work']\n",
      "Stems : ['how', 'car', 'oper']\n",
      "Stems : ['work', 'principl', 'of', 'car']\n",
      "Stems : ['what', 'it', 'take', 'to', 'run', 'a', 'car']\n",
      "Stems : ['compon', 'of', 'car']\n",
      "Stems : ['part', 'of', 'the', 'car']\n",
      "Stems : ['how', 'mani', 'gear', 'car', 'can', 'have']\n",
      "Stems : ['how', 'mani', 'door', 'car', 'can', 'have']\n",
      "Stems : ['how', 'mani', 'peopl', 'can', 'sit', 'in', 'a', 'car']\n",
      "Stems : ['how', 'to', 'chang', 'gear']\n",
      "Stems : ['how', 'to', 'appli', 'revers', 'gear']\n",
      "Stems : ['switch', 'gear']\n",
      "Stems : ['how', 'to', 'revers', 'a', 'car']\n",
      "Stems : ['how', 'to', 'park', 'a', 'car']\n",
      "Stems : ['how', 'mani', 'headlight', 'car', 'have']\n",
      "Stems : ['how', 'mani', 'tail', 'light', 'car', 'can', 'have']\n",
      "Stems : ['how', 'mani', 'indic', 'car', 'have']\n",
      "Stems : ['what', 'is', 'the', 'mileag', 'of', 'your', 'car']\n",
      "Stems : ['averag', 'fuel', 'consumpt']\n",
      "Stems : ['how', 'to', 'start', 'a', 'car']\n",
      "Stems : ['how', 'to', 'stop', 'a', 'car']\n",
      "Stems : ['how', 'to', 'drive', 'a', 'car']\n",
      "Stems : ['tell', 'me', 'about', 'gear']\n",
      "Stems : ['kind', 'of', 'gear', 'car', 'can', 'have']\n",
      "Stems : ['how', 'frequent', 'should', 'serv', 'of', 'car', 'be', 'done']\n",
      "Stems : ['pollut', 'check', 'up']\n",
      "Stems : ['how', 'often', 'should', 'i', 'check', 'my', 'tire', 'pressur']\n",
      "Stems : ['which', 'kind', 'of', 'fuel', 'car', 'use']\n",
      "Stems : ['what', 'provid', 'energi', 'to', 'drive', 'car']\n",
      "Stems : ['how', 'car', 'oper']\n",
      "Stems : ['what', 'kind', 'of', 'transmiss', 'car', 'have']\n",
      "Stems : ['which', 'car', 'is', 'better', 'manual', 'or', 'automat']\n",
      "Stems : ['differnt', 'mode', 'of', 'transmiss']\n",
      "Stems : ['which', 'transmiss', 'type', 'is', 'good']\n",
      "Stems : ['color', 'car', 'have']\n",
      "Stems : ['differ', 'color', 'of', 'car']\n",
      "Stems : ['shade', 'of', 'car']\n",
      "Stems : ['how', 'much', 'we', 'have', 'to', 'invest', 'in', 'repair', 'of', 'a', 'car']\n",
      "Stems : ['maximum', 'speed', 'of', 'car']\n",
      "Stems : ['how', 'can', 'i', 'save', 'money', 'on', 'ga']\n",
      "Stems : ['how', 'much', 'a', 'car', 'cost']\n",
      "Stems : ['price', 'of', 'car']\n",
      "Stems : ['are', 'these', 'autonom', 'vehicl', ',', 'good', 'driver']\n",
      "Stems : ['thing', 'to', 'consid', 'while', 'drive', 'car']\n",
      "Stems : ['tip', 'for', 'first', 'time', 'driver']\n",
      "Stems : ['rule', 'for', 'drive']\n",
      "Stems : ['which', 'is', 'a', 'good', 'four-wheel', 'drive', 'or', 'two', 'drive']\n",
      "Stems : ['what', 'is', 'a', 'four-wheel', 'drive']\n",
      "Stems : ['what', 'is', 'two-wheel', 'drive']\n",
      "Stems : ['where', 'can', 'we', 'drive', 'the', 'car']\n",
      "Stems : ['what', 'happen', 'if', 'someon', 'hit', 'the', 'car']\n",
      "Stems : ['what', 'other', 'amen', 'car', 'have']\n",
      "Stems : ['featur', 'of', 'car']\n",
      "Stems : ['how', 'to', 'acceler', 'a', 'car']\n",
      "Stems : []\n",
      "Stems : ['who', 'invent', 'car']\n",
      "Stems : ['when', 'wa', 'car', 'invent']\n",
      "Stems : ['what', 'is', 'car']\n",
      "Stems : ['fact', 'about', 'car']\n",
      "Stems : ['car']\n",
      "Stems : ['car']\n",
      "Stems : ['how', 'to', 'acceler', 'a', 'car']\n",
      "Stems : ['how', 'to', 'remov', 'the', 'seat', 'belt']\n",
      "Stems : ['do', 'we', 'have', 'to', 'wear', 'seat', 'belt']\n",
      "Stems : ['brake']\n",
      "Stems : ['how', 'to', 'appli', 'the', 'brake']\n",
      "Stems : ['how', 'to', 'use', 'wiper', 'blade']\n",
      "Stems : ['how', 'to', 'turn', 'ac', 'on']\n",
      "Stems : ['turn', 'ac', 'off']\n",
      "Stems : ['ford']\n",
      "Stems : ['tesla']\n",
      "Stems : ['bmw']\n",
      "Stems : ['audi']\n",
      "Stems : ['jeep']\n",
      "Stems : ['ferrari']\n",
      "Stems : ['mustang']\n",
      "Stems : ['honda']\n",
      "Stems : ['tata']\n",
      "Stems : ['bugatta']\n",
      "Stems : ['dodg']\n",
      "Stems : ['toyata']\n",
      "Stems : ['volvo']\n",
      "Stems : ['nissan']\n",
      "Stems : ['buick']\n",
      "Stems : ['chevrolet']\n",
      "Stems : ['chrysler']\n",
      "Stems : ['hyundai']\n",
      "Stems : ['what', 'are', 'variou', 'kind', 'of', 'car']\n",
      "Stems : ['type', 'of', 'car']\n",
      "Stems : ['car', 'list']\n",
      "Stems : ['hi']\n",
      "Stems : ['good', 'day']\n",
      "Stems : ['hello', '!']\n",
      "Stems : ['hey']\n",
      "Stems : ['good', 'morn']\n",
      "Stems : ['gud', 'mrng']\n",
      "Stems : ['hola']\n",
      "Stems : ['who', 'made', 'you']\n",
      "Stems : ['who', 'is', 'your', 'owner']\n",
      "Stems : ['who', 'are', 'your', 'maker']\n",
      "Stems : ['who', 'is', 'your', 'creater']\n",
      "Stems : ['who', 'creat', 'you']\n",
      "Stems : ['manufactur']\n",
      "Stems : ['who', 'built', 'you']\n",
      "Stems : ['how', 'are', 'you', 'abl', 'to', 'answer']\n",
      "Stems : ['how', 'you', 'work']\n",
      "Stems : ['explain', 'your', 'work', 'principl']\n",
      "Stems : ['mechan', 'of', 'work']\n",
      "Stems : ['principl', 'of', 'work']\n",
      "Stems : ['mode', 'of', 'exceut']\n",
      "Stems : ['way', 'of', 'answer']\n",
      "Stems : ['what', 'do', 'you', 'do']\n",
      "Stems : ['how', 'do', 'you', 'talk']\n",
      "Stems : ['how', 'you', 'repli']\n",
      "Stems : ['how', 'you', 'respons']\n",
      "Stems : ['your', 'respons', 'time']\n",
      "Stems : ['how', 'old', 'are', 'you']\n",
      "Stems : ['what', 'is', 'your', 'birth', 'date']\n",
      "Stems : ['when', 'your', 'birthday', 'come']\n",
      "Stems : ['what', 'is', 'your', 'age']\n",
      "Stems : ['age', '?']\n",
      "Stems : ['date', 'of', 'birth']\n",
      "Stems : ['are', 'you', 'talent']\n",
      "Stems : ['how', 'talent', 'you', 'are']\n",
      "Stems : ['what', 'question', 'you', 'can', 'answer']\n",
      "Stems : ['you', 'are', 'expert', 'in']\n",
      "Stems : ['you', 'domain']\n",
      "Stems : ['your', 'context']\n",
      "Stems : ['you', 'are', 'trani', 'in']\n",
      "Stems : ['how', 'to', 'make', 'chatbot']\n",
      "Stems : ['what', 'are', 'your', 'part']\n",
      "Stems : ['main', 'compon']\n",
      "Stems : ['constitu']\n",
      "Stems : ['in', 'which', 'languag', 'you', 'can', 'talk']\n",
      "Stems : ['how', 'you', 'talk']\n",
      "Stems : ['how', 'you', 'speak']\n",
      "Stems : ['you', 'speak', 'in']\n",
      "Stems : ['which', 'languag', 'you', 'use']\n",
      "Stems : ['medium', 'of', 'commun']\n",
      "Stems : ['mode', 'of', 'interact']\n",
      "Stems : ['way', 'of', 'talk']\n",
      "Stems : ['can', 'you', 'speak']\n",
      "Stems : ['what', 'is', 'your', 'name']\n",
      "Stems : ['name']\n",
      "Stems : ['how', 'to', 'call', 'you']\n",
      "Stems : ['what', 'to', 'call', 'you']\n",
      "Stems : ['hey', 'i', 'am']\n",
      "Stems : ['your', 'name']\n",
      "Stems : ['who', 'are', 'you']\n",
      "Stems : ['where', 'to', 'get', 'inform', 'about', 'chatbot']\n",
      "Stems : ['what', 'is', 'chatbot']\n",
      "Stems : ['defin', 'chatbot']\n",
      "Stems : ['creat', 'chatbot']\n",
      "Stems : ['who', 'invent', 'chatbot']\n",
      "Stems : ['first', 'chatbot', 'come', 'in']\n",
      "Stems : ['chatbot']\n",
      "Stems : ['chat']\n",
      "Stems : ['bot']\n",
      "Stems : ['chatbot', 'exit', 'sinc']\n",
      "Stems : ['name', 'of', 'first', 'chatbot']\n",
      "Stems : ['histori', 'of', 'chatbot']\n",
      "Stems : ['use', 'of', 'chatbot']\n",
      "Stems : ['advantag', 'of', 'chatbot']\n",
      "Stems : ['applic', 'of', 'chatbot']\n",
      "Stems : ['pratic', 'use', 'of', 'chatbot']\n",
      "Stems : ['whi', 'should', 'i', 'use', 'you']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems : ['whi', 'to', 'use', 'you']\n",
      "Stems : ['what', 'i', 'will', 'get', 'out', 'of']\n",
      "Stems : ['pro', 'of', 'chatbot']\n",
      "Stems : ['tell', 'me', 'someth', 'bad', 'about', 'you']\n",
      "Stems : ['case', 'where', 'chatbot', 'fail']\n",
      "Stems : ['chatbot']\n",
      "Stems : ['see', 'you', 'later']\n",
      "Stems : ['bye']\n",
      "Stems : ['see', 'you', 'soon']\n",
      "Stems : ['talk', 'to', 'you', 'later']\n",
      "Stems : ['catch', 'up', 'later']\n",
      "Stems : ['will', 'meet', 'again']\n",
      "Stems : ['good', 'bye']\n",
      "Stems : ['will', 'talk', 'later']\n",
      "Stems : ['nice', 'meet', 'talk', 'with', 'you']\n",
      "Stems : ['will', 'catch', 'you', 'later']\n",
      "Stems : ['quit']\n",
      "Stems : ['byye']\n",
      "Stems : ['catch', 'you', 'later']\n",
      "Stems : ['what', \"'s\", 'the', 'weather', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'weather', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'weather', 'now', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'weather', 'today', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'weather', 'now', '?']\n",
      "Stems : ['what', 'is', 'the', 'weather', 'predict', 'today', '?']\n",
      "Stems : ['what', 'is', 'the', 'weather', 'predict', 'now', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'weather', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'weather', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'weather', 'now', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'weather', 'today', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'weather', 'now', '?']\n",
      "Stems : ['weather']\n",
      "Stems : ['climat']\n",
      "Stems : ['will', 'it', 'rain', 'today', '?']\n",
      "Stems : ['is', 'it', 'rain', 'outsid', '?']\n",
      "Stems : ['will', 'it', 'rain', 'now', '?']\n",
      "Stems : ['rain', '?']\n",
      "Stems : ['is', 'it', 'predict', 'to', 'rain', '?']\n",
      "Stems : ['whi', 'it', 'is', 'rain', '?']\n",
      "Stems : ['is', 'there', 'predict', 'shower', 'today', '?']\n",
      "Stems : ['will', 'it', 'shower', 'today', '?']\n",
      "Stems : ['can', 'it', 'shower', 'today', '?']\n",
      "Stems : ['can', 'it', 'rain', 'today', '?']\n",
      "Stems : ['do', 'i', 'need', 'to', 'take', 'umbrella', 'today', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'temperatur', 'now', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'temperatur', 'now', '?']\n",
      "Stems : ['what', 'is', 'the', 'weather', 'predict', 'today', '?']\n",
      "Stems : ['what', 'is', 'the', 'temperatur', 'predict', 'now', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'temperatur', 'now', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'temperatur', 'now', '?']\n",
      "Stems : ['will', 'it', 'snow', 'today', '?']\n",
      "Stems : ['is', 'it', 'snow', 'outsid', '?']\n",
      "Stems : ['will', 'it', 'snow', 'now', '?']\n",
      "Stems : ['snow', '?']\n",
      "Stems : ['is', 'it', 'predict', 'to', 'snow', '?']\n",
      "Stems : ['whi', 'it', 'is', 'snow', '?']\n",
      "Stems : ['is', 'there', 'predict', 'snow', 'today', '?']\n",
      "Stems : ['will', 'it', 'snow', 'today', '?']\n",
      "Stems : ['can', 'it', 'snow', 'today', '?']\n",
      "Stems : ['can', 'it', 'snow', 'today', '?']\n",
      "Stems : ['do', 'i', 'need', 'to', 'take', 'jacket', 'today', '?']\n",
      "Stems : ['can', 'i', 'take', 'jacket', 'today', '?']\n",
      "Stems : ['should', 'i', 'take', 'jacket', 'today', '?']\n",
      "Stems : ['should', 'i', 'layer', 'up', 'today', '?']\n",
      "Stems : ['should', 'i', 'layer', 'up', 'now', '?']\n",
      "Stems : ['is', 'it', 'cloudi', '?']\n",
      "Stems : ['is', 'it', 'cloudi', 'today', '?']\n",
      "Stems : ['it', 'seem', 'cloudi', ',', 'will', 'there', 'be', 'shower', '?']\n",
      "Stems : ['what', 'is', 'the', 'season', 'now', '?']\n",
      "Stems : ['season']\n",
      "Stems : ['season']\n",
      "Stems : ['is', 'it', 'summer']\n",
      "Stems : ['is', 'it', 'winter']\n",
      "Stems : ['is', 'it', 'fall']\n",
      "Stems : ['when', 'is', 'summer']\n",
      "Stems : ['is', 'there', 'posibl', 'of', 'hurrican', '?']\n",
      "Stems : ['hurrican', '?']\n",
      "Stems : ['will', 'hurrican', 'hit', '?']\n",
      "Stems : ['is', 'there', 'a', 'posibl', 'of', 'snow', 'storm', '?']\n",
      "Stems : ['snowstorm', '?']\n",
      "Stems : ['snow', 'storm', '?']\n",
      "Stems : ['is', 'there', 'a', 'posibl', 'of', 'storm', '?']\n",
      "Stems : ['is', 'there', 'possibl', 'of', 'cyclon', '?']\n",
      "Stems : ['will', 'cyclon', 'hit', 'us', '?']\n",
      "Stems : ['ani', 'possibl', 'of', 'cyclon', 'soon', '?']\n",
      "Stems : ['is', 'ani', 'calam', 'possibl', 'now', '?']\n",
      "Train on 308 samples\n",
      "Epoch 1/300\n",
      "308/308 [==============================] - ETA: 12s - loss: 3.89 - 1s 5ms/sample - loss: 3.9079\n",
      "Epoch 2/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.906 - 0s 117us/sample - loss: 3.8978\n",
      "Epoch 3/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.900 - 0s 84us/sample - loss: 3.8882\n",
      "Epoch 4/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.891 - 0s 104us/sample - loss: 3.8768\n",
      "Epoch 5/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.857 - 0s 123us/sample - loss: 3.8625\n",
      "Epoch 6/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.839 - 0s 104us/sample - loss: 3.8431\n",
      "Epoch 7/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.863 - 0s 97us/sample - loss: 3.8159\n",
      "Epoch 8/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.798 - 0s 120us/sample - loss: 3.7807\n",
      "Epoch 9/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.741 - 0s 140us/sample - loss: 3.7360\n",
      "Epoch 10/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.703 - 0s 143us/sample - loss: 3.6831\n",
      "Epoch 11/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.598 - 0s 127us/sample - loss: 3.6236\n",
      "Epoch 12/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.485 - 0s 130us/sample - loss: 3.5585\n",
      "Epoch 13/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.515 - 0s 143us/sample - loss: 3.4907\n",
      "Epoch 14/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.553 - 0s 104us/sample - loss: 3.4268\n",
      "Epoch 15/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.344 - 0s 110us/sample - loss: 3.3623\n",
      "Epoch 16/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.144 - 0s 110us/sample - loss: 3.3027\n",
      "Epoch 17/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.136 - 0s 97us/sample - loss: 3.2485\n",
      "Epoch 18/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.235 - 0s 127us/sample - loss: 3.1936\n",
      "Epoch 19/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.221 - 0s 110us/sample - loss: 3.1439\n",
      "Epoch 20/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.285 - 0s 130us/sample - loss: 3.0927\n",
      "Epoch 21/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.938 - 0s 114us/sample - loss: 3.0424\n",
      "Epoch 22/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.779 - 0s 120us/sample - loss: 2.9932\n",
      "Epoch 23/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.813 - 0s 97us/sample - loss: 2.9432\n",
      "Epoch 24/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.910 - 0s 110us/sample - loss: 2.8935\n",
      "Epoch 25/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.552 - 0s 130us/sample - loss: 2.8432\n",
      "Epoch 26/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.951 - 0s 117us/sample - loss: 2.7937\n",
      "Epoch 27/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 3.117 - 0s 149us/sample - loss: 2.7412\n",
      "Epoch 28/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.562 - 0s 140us/sample - loss: 2.6915\n",
      "Epoch 29/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.855 - 0s 127us/sample - loss: 2.6388\n",
      "Epoch 30/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.953 - 0s 117us/sample - loss: 2.5856\n",
      "Epoch 31/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.718 - 0s 123us/sample - loss: 2.5333\n",
      "Epoch 32/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.431 - 0s 107us/sample - loss: 2.4790\n",
      "Epoch 33/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.352 - 0s 123us/sample - loss: 2.4273\n",
      "Epoch 34/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.591 - 0s 81us/sample - loss: 2.3726\n",
      "Epoch 35/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.260 - 0s 107us/sample - loss: 2.3196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.997 - 0s 81us/sample - loss: 2.2646\n",
      "Epoch 37/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.534 - 0s 110us/sample - loss: 2.2107\n",
      "Epoch 38/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.360 - 0s 143us/sample - loss: 2.1565\n",
      "Epoch 39/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.338 - 0s 101us/sample - loss: 2.0993\n",
      "Epoch 40/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.783 - 0s 101us/sample - loss: 2.0455\n",
      "Epoch 41/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.094 - 0s 114us/sample - loss: 1.9914\n",
      "Epoch 42/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.014 - 0s 91us/sample - loss: 1.9380\n",
      "Epoch 43/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.845 - 0s 110us/sample - loss: 1.8858\n",
      "Epoch 44/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.869 - 0s 81us/sample - loss: 1.8359\n",
      "Epoch 45/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 2.017 - 0s 130us/sample - loss: 1.7821\n",
      "Epoch 46/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.841 - 0s 104us/sample - loss: 1.7313\n",
      "Epoch 47/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.885 - 0s 84us/sample - loss: 1.6831\n",
      "Epoch 48/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.302 - 0s 78us/sample - loss: 1.6334\n",
      "Epoch 49/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.346 - 0s 78us/sample - loss: 1.5853\n",
      "Epoch 50/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.447 - 0s 94us/sample - loss: 1.5399\n",
      "Epoch 51/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.436 - 0s 94us/sample - loss: 1.4921\n",
      "Epoch 52/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.567 - 0s 81us/sample - loss: 1.4453\n",
      "Epoch 53/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.307 - 0s 91us/sample - loss: 1.4041\n",
      "Epoch 54/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.243 - 0s 117us/sample - loss: 1.3614\n",
      "Epoch 55/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.437 - 0s 192us/sample - loss: 1.3178\n",
      "Epoch 56/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.100 - 0s 117us/sample - loss: 1.2773\n",
      "Epoch 57/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.832 - 0s 146us/sample - loss: 1.2380\n",
      "Epoch 58/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.566 - 0s 169us/sample - loss: 1.1976\n",
      "Epoch 59/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.032 - 0s 156us/sample - loss: 1.1598\n",
      "Epoch 60/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.075 - 0s 156us/sample - loss: 1.1220\n",
      "Epoch 61/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.425 - 0s 110us/sample - loss: 1.0853\n",
      "Epoch 62/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.818 - 0s 218us/sample - loss: 1.0507\n",
      "Epoch 63/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.073 - 0s 153us/sample - loss: 1.0163\n",
      "Epoch 64/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.051 - 0s 153us/sample - loss: 0.9828\n",
      "Epoch 65/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.863 - 0s 146us/sample - loss: 0.9513\n",
      "Epoch 66/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.023 - 0s 143us/sample - loss: 0.9176\n",
      "Epoch 67/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.157 - 0s 162us/sample - loss: 0.8883\n",
      "Epoch 68/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.691 - 0s 127us/sample - loss: 0.8601\n",
      "Epoch 69/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.920 - 0s 94us/sample - loss: 0.8327\n",
      "Epoch 70/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.908 - 0s 107us/sample - loss: 0.8064\n",
      "Epoch 71/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.722 - 0s 149us/sample - loss: 0.7783\n",
      "Epoch 72/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.648 - 0s 136us/sample - loss: 0.7550\n",
      "Epoch 73/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.769 - 0s 110us/sample - loss: 0.7314\n",
      "Epoch 74/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.643 - 0s 127us/sample - loss: 0.7064\n",
      "Epoch 75/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.817 - 0s 166us/sample - loss: 0.6815\n",
      "Epoch 76/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.612 - 0s 107us/sample - loss: 0.6602\n",
      "Epoch 77/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.677 - 0s 120us/sample - loss: 0.6404\n",
      "Epoch 78/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.486 - 0s 84us/sample - loss: 0.6170\n",
      "Epoch 79/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.526 - 0s 75us/sample - loss: 0.5983\n",
      "Epoch 80/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.726 - 0s 123us/sample - loss: 0.5775\n",
      "Epoch 81/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.593 - 0s 123us/sample - loss: 0.5600\n",
      "Epoch 82/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.489 - 0s 140us/sample - loss: 0.5405\n",
      "Epoch 83/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.487 - 0s 123us/sample - loss: 0.5238\n",
      "Epoch 84/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.485 - 0s 140us/sample - loss: 0.5070\n",
      "Epoch 85/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.440 - ETA: 0s - loss: 0.493 - 0s 240us/sample - loss: 0.4906\n",
      "Epoch 86/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.365 - 0s 156us/sample - loss: 0.4775\n",
      "Epoch 87/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.307 - 0s 146us/sample - loss: 0.4615\n",
      "Epoch 88/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.516 - 0s 114us/sample - loss: 0.4479\n",
      "Epoch 89/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.235 - 0s 114us/sample - loss: 0.4337\n",
      "Epoch 90/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.465 - 0s 101us/sample - loss: 0.4221\n",
      "Epoch 91/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.274 - 0s 133us/sample - loss: 0.4087\n",
      "Epoch 92/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.258 - 0s 133us/sample - loss: 0.3949\n",
      "Epoch 93/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.322 - 0s 130us/sample - loss: 0.3853\n",
      "Epoch 94/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.341 - 0s 136us/sample - loss: 0.3729\n",
      "Epoch 95/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.425 - 0s 117us/sample - loss: 0.3619\n",
      "Epoch 96/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.295 - 0s 133us/sample - loss: 0.3507\n",
      "Epoch 97/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.277 - 0s 123us/sample - loss: 0.3409\n",
      "Epoch 98/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.277 - 0s 117us/sample - loss: 0.3303\n",
      "Epoch 99/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.278 - 0s 107us/sample - loss: 0.3237\n",
      "Epoch 100/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.311 - 0s 114us/sample - loss: 0.3133\n",
      "Epoch 101/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.293 - 0s 101us/sample - loss: 0.3031\n",
      "Epoch 102/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.321 - 0s 91us/sample - loss: 0.2951\n",
      "Epoch 103/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.308 - 0s 71us/sample - loss: 0.2863\n",
      "Epoch 104/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.326 - 0s 78us/sample - loss: 0.2774\n",
      "Epoch 105/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.318 - 0s 81us/sample - loss: 0.2713\n",
      "Epoch 106/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.266 - 0s 94us/sample - loss: 0.2615\n",
      "Epoch 107/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.110 - 0s 136us/sample - loss: 0.2550\n",
      "Epoch 108/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.281 - 0s 110us/sample - loss: 0.2481\n",
      "Epoch 109/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - ETA: 0s - loss: 0.352 - 0s 101us/sample - loss: 0.2421\n",
      "Epoch 110/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.103 - 0s 94us/sample - loss: 0.2362\n",
      "Epoch 111/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.338 - 0s 127us/sample - loss: 0.2316\n",
      "Epoch 112/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.151 - 0s 130us/sample - loss: 0.2235\n",
      "Epoch 113/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.198 - 0s 101us/sample - loss: 0.2189\n",
      "Epoch 114/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.220 - 0s 153us/sample - loss: 0.2129\n",
      "Epoch 115/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.186 - 0s 117us/sample - loss: 0.2079\n",
      "Epoch 116/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.263 - 0s 107us/sample - loss: 0.2029\n",
      "Epoch 117/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.181 - 0s 123us/sample - loss: 0.1948\n",
      "Epoch 118/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.200 - 0s 133us/sample - loss: 0.1907\n",
      "Epoch 119/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.259 - 0s 130us/sample - loss: 0.1841\n",
      "Epoch 120/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.190 - 0s 110us/sample - loss: 0.1805\n",
      "Epoch 121/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.105 - 0s 110us/sample - loss: 0.1768\n",
      "Epoch 122/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.093 - 0s 97us/sample - loss: 0.1709\n",
      "Epoch 123/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.190 - 0s 114us/sample - loss: 0.1685\n",
      "Epoch 124/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.170 - 0s 123us/sample - loss: 0.1625\n",
      "Epoch 125/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.211 - 0s 110us/sample - loss: 0.1597\n",
      "Epoch 126/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.197 - 0s 97us/sample - loss: 0.1548\n",
      "Epoch 127/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.128 - 0s 78us/sample - loss: 0.1502\n",
      "Epoch 128/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.101 - 0s 97us/sample - loss: 0.1468\n",
      "Epoch 129/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.169 - 0s 84us/sample - loss: 0.1450\n",
      "Epoch 130/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.173 - 0s 75us/sample - loss: 0.1406\n",
      "Epoch 131/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.148 - 0s 81us/sample - loss: 0.1376\n",
      "Epoch 132/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.179 - 0s 75us/sample - loss: 0.1361\n",
      "Epoch 133/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.108 - 0s 84us/sample - loss: 0.1324\n",
      "Epoch 134/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.128 - 0s 104us/sample - loss: 0.1280\n",
      "Epoch 135/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.051 - 0s 136us/sample - loss: 0.1274\n",
      "Epoch 136/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.208 - 0s 172us/sample - loss: 0.1231\n",
      "Epoch 137/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.093 - 0s 130us/sample - loss: 0.1209\n",
      "Epoch 138/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.114 - 0s 156us/sample - loss: 0.1197\n",
      "Epoch 139/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.127 - 0s 123us/sample - loss: 0.1176\n",
      "Epoch 140/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.116 - 0s 107us/sample - loss: 0.1136\n",
      "Epoch 141/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.137 - 0s 81us/sample - loss: 0.1115\n",
      "Epoch 142/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.142 - 0s 101us/sample - loss: 0.1092\n",
      "Epoch 143/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.133 - 0s 88us/sample - loss: 0.1069\n",
      "Epoch 144/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.113 - 0s 78us/sample - loss: 0.1077\n",
      "Epoch 145/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.050 - 0s 91us/sample - loss: 0.1029\n",
      "Epoch 146/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.098 - 0s 149us/sample - loss: 0.1011\n",
      "Epoch 147/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.080 - 0s 143us/sample - loss: 0.0996\n",
      "Epoch 148/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.098 - 0s 127us/sample - loss: 0.0995\n",
      "Epoch 149/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.036 - 0s 123us/sample - loss: 0.0962\n",
      "Epoch 150/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.114 - 0s 71us/sample - loss: 0.0949\n",
      "Epoch 151/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.061 - 0s 91us/sample - loss: 0.0922\n",
      "Epoch 152/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.076 - 0s 71us/sample - loss: 0.0921\n",
      "Epoch 153/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.122 - 0s 75us/sample - loss: 0.0900\n",
      "Epoch 154/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.139 - 0s 91us/sample - loss: 0.0875\n",
      "Epoch 155/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.120 - 0s 101us/sample - loss: 0.0871\n",
      "Epoch 156/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.097 - 0s 101us/sample - loss: 0.0861\n",
      "Epoch 157/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.070 - 0s 123us/sample - loss: 0.0831\n",
      "Epoch 158/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.115 - 0s 156us/sample - loss: 0.0826\n",
      "Epoch 159/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.144 - 0s 97us/sample - loss: 0.0817\n",
      "Epoch 160/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.055 - 0s 120us/sample - loss: 0.0789\n",
      "Epoch 161/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.079 - 0s 123us/sample - loss: 0.0790\n",
      "Epoch 162/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.122 - 0s 117us/sample - loss: 0.0775\n",
      "Epoch 163/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.068 - 0s 127us/sample - loss: 0.0771\n",
      "Epoch 164/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.108 - 0s 130us/sample - loss: 0.0755\n",
      "Epoch 165/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.059 - 0s 107us/sample - loss: 0.0748\n",
      "Epoch 166/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.095 - 0s 149us/sample - loss: 0.0724\n",
      "Epoch 167/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.066 - 0s 110us/sample - loss: 0.0729\n",
      "Epoch 168/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.043 - 0s 107us/sample - loss: 0.0721\n",
      "Epoch 169/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.058 - 0s 88us/sample - loss: 0.0696\n",
      "Epoch 170/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.076 - 0s 71us/sample - loss: 0.0699\n",
      "Epoch 171/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.085 - 0s 101us/sample - loss: 0.0699\n",
      "Epoch 172/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.062 - 0s 68us/sample - loss: 0.0682\n",
      "Epoch 173/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.035 - 0s 97us/sample - loss: 0.0670\n",
      "Epoch 174/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.058 - 0s 130us/sample - loss: 0.0662\n",
      "Epoch 175/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.031 - 0s 88us/sample - loss: 0.0648\n",
      "Epoch 176/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.053 - 0s 107us/sample - loss: 0.0637\n",
      "Epoch 177/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.051 - 0s 101us/sample - loss: 0.0633\n",
      "Epoch 178/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.067 - 0s 91us/sample - loss: 0.0630\n",
      "Epoch 179/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.069 - 0s 94us/sample - loss: 0.0623\n",
      "Epoch 180/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.063 - 0s 91us/sample - loss: 0.0616\n",
      "Epoch 181/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.050 - 0s 104us/sample - loss: 0.0616\n",
      "Epoch 182/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - ETA: 0s - loss: 0.089 - 0s 94us/sample - loss: 0.0596\n",
      "Epoch 183/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.083 - 0s 101us/sample - loss: 0.0596\n",
      "Epoch 184/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.080 - 0s 88us/sample - loss: 0.0593\n",
      "Epoch 185/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.089 - 0s 101us/sample - loss: 0.0613\n",
      "Epoch 186/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.038 - 0s 140us/sample - loss: 0.0570\n",
      "Epoch 187/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.052 - 0s 120us/sample - loss: 0.0582\n",
      "Epoch 188/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.051 - 0s 104us/sample - loss: 0.0547\n",
      "Epoch 189/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.023 - 0s 101us/sample - loss: 0.0565\n",
      "Epoch 190/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.080 - 0s 107us/sample - loss: 0.0546\n",
      "Epoch 191/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.063 - 0s 114us/sample - loss: 0.0560\n",
      "Epoch 192/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.128 - 0s 104us/sample - loss: 0.0577\n",
      "Epoch 193/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.066 - 0s 101us/sample - loss: 0.0532\n",
      "Epoch 194/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.030 - 0s 88us/sample - loss: 0.0524\n",
      "Epoch 195/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.046 - 0s 117us/sample - loss: 0.0526\n",
      "Epoch 196/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.054 - 0s 110us/sample - loss: 0.0525\n",
      "Epoch 197/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.020 - 0s 169us/sample - loss: 0.0524\n",
      "Epoch 198/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.065 - 0s 127us/sample - loss: 0.0514\n",
      "Epoch 199/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.093 - 0s 114us/sample - loss: 0.0508\n",
      "Epoch 200/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.076 - 0s 143us/sample - loss: 0.0507\n",
      "Epoch 201/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.073 - 0s 123us/sample - loss: 0.0508\n",
      "Epoch 202/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.051 - 0s 130us/sample - loss: 0.0499\n",
      "Epoch 203/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.012 - 0s 146us/sample - loss: 0.0477\n",
      "Epoch 204/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.035 - 0s 149us/sample - loss: 0.0485\n",
      "Epoch 205/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.082 - 0s 127us/sample - loss: 0.0471\n",
      "Epoch 206/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.038 - 0s 123us/sample - loss: 0.0482\n",
      "Epoch 207/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.085 - 0s 123us/sample - loss: 0.0466\n",
      "Epoch 208/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.018 - 0s 114us/sample - loss: 0.0476\n",
      "Epoch 209/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.027 - 0s 97us/sample - loss: 0.0478\n",
      "Epoch 210/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.024 - 0s 123us/sample - loss: 0.0450\n",
      "Epoch 211/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.020 - 0s 136us/sample - loss: 0.0462\n",
      "Epoch 212/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.037 - 0s 101us/sample - loss: 0.0466\n",
      "Epoch 213/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.040 - 0s 88us/sample - loss: 0.0458\n",
      "Epoch 214/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.048 - 0s 81us/sample - loss: 0.0452\n",
      "Epoch 215/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.040 - 0s 81us/sample - loss: 0.0441\n",
      "Epoch 216/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.016 - 0s 94us/sample - loss: 0.0440\n",
      "Epoch 217/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.016 - 0s 97us/sample - loss: 0.0444\n",
      "Epoch 218/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.016 - 0s 101us/sample - loss: 0.0436\n",
      "Epoch 219/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.084 - 0s 101us/sample - loss: 0.0437\n",
      "Epoch 220/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.042 - 0s 133us/sample - loss: 0.0437\n",
      "Epoch 221/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.049 - 0s 71us/sample - loss: 0.0427\n",
      "Epoch 222/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.034 - 0s 107us/sample - loss: 0.0434\n",
      "Epoch 223/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.011 - 0s 101us/sample - loss: 0.0424\n",
      "Epoch 224/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.082 - 0s 104us/sample - loss: 0.0434\n",
      "Epoch 225/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.075 - 0s 114us/sample - loss: 0.0415\n",
      "Epoch 226/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.013 - 0s 88us/sample - loss: 0.0418\n",
      "Epoch 227/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.015 - 0s 104us/sample - loss: 0.0426\n",
      "Epoch 228/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.072 - 0s 101us/sample - loss: 0.0403\n",
      "Epoch 229/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.051 - 0s 91us/sample - loss: 0.0430\n",
      "Epoch 230/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.015 - 0s 107us/sample - loss: 0.0427\n",
      "Epoch 231/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.060 - 0s 133us/sample - loss: 0.0415\n",
      "Epoch 232/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.070 - 0s 133us/sample - loss: 0.0431\n",
      "Epoch 233/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.046 - 0s 136us/sample - loss: 0.0412\n",
      "Epoch 234/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.041 - 0s 130us/sample - loss: 0.0395\n",
      "Epoch 235/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.016 - 0s 140us/sample - loss: 0.0402\n",
      "Epoch 236/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.036 - 0s 120us/sample - loss: 0.0394\n",
      "Epoch 237/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.036 - 0s 97us/sample - loss: 0.0396\n",
      "Epoch 238/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.046 - 0s 81us/sample - loss: 0.0390\n",
      "Epoch 239/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.102 - 0s 84us/sample - loss: 0.0399\n",
      "Epoch 240/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.032 - 0s 68us/sample - loss: 0.0410\n",
      "Epoch 241/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.012 - 0s 75us/sample - loss: 0.0389\n",
      "Epoch 242/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.015 - 0s 81us/sample - loss: 0.0383\n",
      "Epoch 243/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.027 - 0s 91us/sample - loss: 0.0392\n",
      "Epoch 244/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.014 - 0s 97us/sample - loss: 0.0372\n",
      "Epoch 245/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.030 - 0s 110us/sample - loss: 0.0389\n",
      "Epoch 246/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.120 - 0s 91us/sample - loss: 0.0404\n",
      "Epoch 247/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.067 - 0s 110us/sample - loss: 0.0375\n",
      "Epoch 248/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.052 - 0s 104us/sample - loss: 0.0370\n",
      "Epoch 249/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.024 - 0s 97us/sample - loss: 0.0381\n",
      "Epoch 250/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.052 - 0s 101us/sample - loss: 0.0361\n",
      "Epoch 251/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.036 - 0s 104us/sample - loss: 0.0379\n",
      "Epoch 252/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.080 - 0s 91us/sample - loss: 0.0381\n",
      "Epoch 253/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.061 - 0s 114us/sample - loss: 0.0424\n",
      "Epoch 254/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.012 - 0s 97us/sample - loss: 0.0349\n",
      "Epoch 255/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - ETA: 0s - loss: 0.026 - 0s 97us/sample - loss: 0.0379\n",
      "Epoch 256/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.016 - 0s 71us/sample - loss: 0.0376\n",
      "Epoch 257/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.051 - 0s 78us/sample - loss: 0.0371\n",
      "Epoch 258/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.011 - 0s 104us/sample - loss: 0.0360\n",
      "Epoch 259/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.013 - 0s 120us/sample - loss: 0.0368\n",
      "Epoch 260/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.024 - 0s 91us/sample - loss: 0.0371\n",
      "Epoch 261/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.010 - 0s 104us/sample - loss: 0.0359\n",
      "Epoch 262/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.009 - 0s 94us/sample - loss: 0.0357\n",
      "Epoch 263/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.026 - 0s 101us/sample - loss: 0.0363\n",
      "Epoch 264/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.012 - 0s 101us/sample - loss: 0.0354\n",
      "Epoch 265/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.027 - 0s 104us/sample - loss: 0.0342\n",
      "Epoch 266/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.033 - 0s 101us/sample - loss: 0.0355\n",
      "Epoch 267/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.070 - 0s 101us/sample - loss: 0.0369\n",
      "Epoch 268/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.058 - 0s 107us/sample - loss: 0.0364\n",
      "Epoch 269/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.033 - 0s 104us/sample - loss: 0.0363\n",
      "Epoch 270/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.009 - 0s 107us/sample - loss: 0.0346\n",
      "Epoch 271/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.042 - 0s 110us/sample - loss: 0.0334\n",
      "Epoch 272/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.053 - 0s 97us/sample - loss: 0.0356\n",
      "Epoch 273/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0346\n",
      "Epoch 274/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.060 - 0s 88us/sample - loss: 0.0347\n",
      "Epoch 275/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0328\n",
      "Epoch 276/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.034 - 0s 107us/sample - loss: 0.0332\n",
      "Epoch 277/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.067 - 0s 101us/sample - loss: 0.0340\n",
      "Epoch 278/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.048 - 0s 101us/sample - loss: 0.0337\n",
      "Epoch 279/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.028 - 0s 97us/sample - loss: 0.0357\n",
      "Epoch 280/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.090 - 0s 117us/sample - loss: 0.0346\n",
      "Epoch 281/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.030 - 0s 84us/sample - loss: 0.0332\n",
      "Epoch 282/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.043 - 0s 78us/sample - loss: 0.0345\n",
      "Epoch 283/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.057 - 0s 97us/sample - loss: 0.0334\n",
      "Epoch 284/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.007 - 0s 97us/sample - loss: 0.0334\n",
      "Epoch 285/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.010 - 0s 94us/sample - loss: 0.0341\n",
      "Epoch 286/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.049 - 0s 114us/sample - loss: 0.0346\n",
      "Epoch 287/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.031 - 0s 104us/sample - loss: 0.0347\n",
      "Epoch 288/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.042 - 0s 101us/sample - loss: 0.0338\n",
      "Epoch 289/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.028 - 0s 84us/sample - loss: 0.0325\n",
      "Epoch 290/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.049 - 0s 94us/sample - loss: 0.0335\n",
      "Epoch 291/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.034 - 0s 97us/sample - loss: 0.0346\n",
      "Epoch 292/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.011 - 0s 97us/sample - loss: 0.0336\n",
      "Epoch 293/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.007 - 0s 94us/sample - loss: 0.0326\n",
      "Epoch 294/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.023 - 0s 91us/sample - loss: 0.0319\n",
      "Epoch 295/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0317\n",
      "Epoch 296/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.034 - 0s 97us/sample - loss: 0.0333\n",
      "Epoch 297/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.010 - 0s 107us/sample - loss: 0.0315\n",
      "Epoch 298/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0328\n",
      "Epoch 299/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.009 - 0s 81us/sample - loss: 0.0314\n",
      "Epoch 300/300\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.007 - 0s 114us/sample - loss: 0.0328\n",
      "Start talking with the bot (type quit to stop)!\n",
      "You: yes\n",
      "Stems : ['ye']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.59356517\n",
      "Intent of sentence :  Color\n",
      "chatbot: thers are many colors of car like black, orange, yellow, brown, blue, silver\n",
      "You: yes\n",
      "Stems : ['ye']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.59356517\n",
      "Intent of sentence :  Color\n",
      "chatbot: thers are many colors of car like black, orange, yellow, brown, blue, silver\n",
      "You: yes\n",
      "Stems : ['ye']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.59356517\n",
      "Intent of sentence :  Color\n",
      "chatbot: thers are many colors of car like black, orange, yellow, brown, blue, silver\n",
      "You: yes\n",
      "Stems : ['ye']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.59356517\n",
      "Intent of sentence :  Color\n",
      "chatbot: thers are many colors of car like black, orange, yellow, brown, blue, silver\n",
      "You: weather\n",
      "Stems : ['weather']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.95506805\n",
      "Intent of sentence :  Weather\n",
      "chatbot: it is predicted to be Sunny and bright\n",
      "You: sunny\n",
      "Stems : ['sunni']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.59356517\n",
      "Intent of sentence :  Color\n",
      "chatbot: thers are many colors of car like black, orange, yellow, brown, blue, silver\n",
      "You: brake\n",
      "Stems : ['brake']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9893758\n",
      "Intent of sentence :  ReadManual\n",
      "chatbot: Please read the manual\n",
      "You: how to aplly brake\n",
      "Stems : ['how', 'to', 'aplli', 'brake']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.99815255\n",
      "Intent of sentence :  ReadManual\n",
      "chatbot: Please read the manual\n",
      "You: how to buy car\n",
      "Stems : ['how', 'to', 'buy', 'car']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.8791012\n",
      "Intent of sentence :  Facts\n",
      "chatbot: Karl Benz invented in 1885, car is a wheeled motor vehicle used for transportation\n",
      "You: ford\n",
      "Stems : ['ford']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9964796\n",
      "Intent of sentence :  CarNames\n",
      "chatbot: SUV, sedan, Hatchback, MPV, crossover,couple and convertible\n",
      "You: hyundai\n",
      "Stems : ['hyundai']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9957231\n",
      "Intent of sentence :  CarNames\n",
      "chatbot: SUV, sedan, Hatchback, MPV, crossover,couple and convertible\n",
      "You: gas\n",
      "Stems : ['ga']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.46540833\n",
      "Intent of sentence :  Advantages\n",
      "chatbot: you can travel faster and comfortably\n",
      "You: how to save gas\n",
      "Stems : ['how', 'to', 'save', 'ga']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.96388686\n",
      "Intent of sentence :  SubjectiveAnswers\n",
      "chatbot: No common answer as it varies\n",
      "You: how to accelerate\n",
      "Stems : ['how', 'to', 'acceler']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.959158\n",
      "Intent of sentence :  ReadManual\n",
      "chatbot: Please read the manual\n",
      "You:  top cars\n",
      "Stems : ['top', 'car']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9595719\n",
      "Intent of sentence :  Facts\n",
      "chatbot: Karl Benz invented in 1885, car is a wheeled motor vehicle used for transportation\n",
      "You: luxery ar\n",
      "Stems : ['luxeri', 'ar']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.59356517\n",
      "Intent of sentence :  Color\n",
      "chatbot: thers are many colors of car like black, orange, yellow, brown, blue, silver\n",
      "You: expensive care\n",
      "Stems : ['expens', 'care']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.7566119\n",
      "Intent of sentence :  Facts\n",
      "chatbot: Karl Benz invented in 1885, car is a wheeled motor vehicle used for transportation\n",
      "You: expensive car\n",
      "Stems : ['expens', 'car']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.983204\n",
      "Intent of sentence :  TopCars\n",
      "chatbot: Audi Q3\n",
      "You: does car have 4 wheels\n",
      "Stems : ['doe', 'car', 'have', '4', 'wheel']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9751439\n",
      "Intent of sentence :  PopularCars\n",
      "chatbot: Volkswagen Golf\n",
      "You: what is car\n",
      "Stems : ['what', 'is', 'car']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9527228\n",
      "Intent of sentence :  Facts\n",
      "chatbot: Karl Benz invented in 1885, car is a wheeled motor vehicle used for transportation\n",
      "You: most famous cars\n",
      "Stems : ['most', 'famou', 'car']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9595719\n",
      "Intent of sentence :  Facts\n",
      "chatbot: Karl Benz invented in 1885, car is a wheeled motor vehicle used for transportation\n",
      "You: can women drive car\n",
      "Stems : ['can', 'women', 'drive', 'car']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Highest intent proability: 0.9904841\n",
      "Intent of sentence :  Yes\n",
      "chatbot: Yes\n",
      "You: can girl drive car\n",
      "Stems : ['can', 'girl', 'drive', 'car']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9770511\n",
      "Intent of sentence :  Yes\n",
      "chatbot: Yes\n",
      "You: can preethi drive car\n",
      "Stems : ['can', 'preethi', 'drive', 'car']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9770511\n",
      "Intent of sentence :  Yes\n",
      "chatbot: Yes\n",
      "You: can monkey drive car\n",
      "Stems : ['can', 'monkey', 'drive', 'car']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9770511\n",
      "Intent of sentence :  Yes\n",
      "chatbot: Yes\n",
      "You: can rat drive car\n",
      "Stems : ['can', 'rat', 'drive', 'car']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.9770511\n",
      "Intent of sentence :  Yes\n",
      "chatbot: Yes\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import h5py\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import spacy\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "class Chatbot:\n",
    "\n",
    "    showWorking = False\n",
    "    train_again=False\n",
    "    vocabulary=[]\n",
    "    labels=[]\n",
    "    responseDictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    def __init__(self,train_again=False):\n",
    "        Chatbot.train_again = train_again\n",
    "        \n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    customize_stop_words = ['.', '?','!']\n",
    "    for w in customize_stop_words:\n",
    "        spacy_nlp.vocab[w].is_stop = True\n",
    "\n",
    "    def get_model(self):\n",
    "\n",
    "        if not Chatbot.train_again:\n",
    "            with open ('LabelList.json') as fl:\n",
    "                Chatbot.labels = json.load(fl)\n",
    "            \n",
    "            with open ('VocabularyList.json') as fv:\n",
    "                Chatbot.vocabulary = json.load(fv)\n",
    "                \n",
    "            with open ('responseDictionary.json') as fr:\n",
    "                Chatbot.responseDictionary = json.load(fr)\n",
    "                \n",
    "            model= tf.keras.models.load_model(\"chatBotold.h5\")\n",
    "        else:\n",
    "            with open('CommonIntentions.json') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            docs_x = []\n",
    "            docs_y = []\n",
    "\n",
    "            for intent in data['intents']:\n",
    "                \n",
    "                Chatbot.responseDictionary[intent['tag']] = []\n",
    "                responseList=[]\n",
    "                for response in intent['responses']:\n",
    "                    responseList.append(response)\n",
    "                    \n",
    "                Chatbot.responseDictionary[intent['tag']] = responseList\n",
    "                \n",
    "                for pattern in intent['patterns']:\n",
    "                    wrds = remove_stop_words_get_lemmas(pattern)\n",
    "                    if len(wrds)>0:\n",
    "                        Chatbot.vocabulary.extend(wrds)\n",
    "                        docs_x.append(wrds)\n",
    "                        docs_y.append(intent['tag'])               \n",
    "\n",
    "                if intent['tag'] not in Chatbot.labels:\n",
    "                    Chatbot.labels.append(intent['tag'])\n",
    "                    \n",
    "            with open('responseDictionary.json', 'w') as fp:\n",
    "                json.dump(Chatbot.responseDictionary, fp)\n",
    "                \n",
    "            Chatbot.vocabulary = sorted(list(set(Chatbot.vocabulary)))\n",
    "            Chatbot.labels = sorted(Chatbot.labels)\n",
    "            training = []\n",
    "            output = []\n",
    "            out_empty = [0 for _ in range(len(Chatbot.labels))]\n",
    "\n",
    "            for x, doc in enumerate(docs_x):\n",
    "                bag = []\n",
    "\n",
    "                for w in Chatbot.vocabulary:\n",
    "                    if w in doc:\n",
    "                        bag.append(1)\n",
    "                    else:\n",
    "                        bag.append(0)\n",
    "\n",
    "                output_row = out_empty[:]\n",
    "                output_row[Chatbot.labels.index(docs_y[x])] = 1\n",
    "\n",
    "                training.append(bag)\n",
    "                output.append(output_row)\n",
    "\n",
    "            training = numpy.array(training)\n",
    "            output = numpy.array(output)\n",
    "\n",
    "\n",
    "            model = tf.keras.Sequential()\n",
    "            for layer in range(3):\n",
    "            # Adds a densely-connected layer with specified units to the model:\n",
    "                model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "\n",
    "        # Add a sigmoid layer with 1 output units:\n",
    "            model.add(tf.keras.layers.Dense(len(output[0]), activation='softmax'))\n",
    "            model.compile(optimizer='adam',loss='categorical_crossentropy',metric=['accuracy'])\n",
    "            model.fit(training, output, epochs=300)\n",
    "            model.save(\"chatBotold.h5\")\n",
    "            \n",
    "            with open('LabelList.json', 'w') as fl:\n",
    "                json.dump(Chatbot.labels, fl)\n",
    "            with open('VocabularyList.json', 'w') as fv:\n",
    "                json.dump(Chatbot.vocabulary, fv)\n",
    "        return model\n",
    "\n",
    "    def get_yes_list(self):\n",
    "        return ['yes','sure','ok','okay','for sure','sure','totally','yep','ya','yeah','yup','certainly',\n",
    "                         'definitely','of course','gladly','absolutely','indeed']\n",
    "    \n",
    "    def chat(self):\n",
    "        \n",
    "        print(\"Do you want to see internal processing steps:\")\n",
    "        choice = input(\"Enter you choice : \")\n",
    "        if choice.lower() in self.get_yes_list():\n",
    "            Chatbot.showWorking=True\n",
    "        model = self.get_model()\n",
    "        print(\"Start talking with the bot (type quit to stop)!\")\n",
    "        while True:\n",
    "            inp = input(\"You: \")\n",
    "            if inp.lower() == \"quit\":\n",
    "                break\n",
    "            results = model.predict(bag_of_words(inp))\n",
    "            results_index = numpy.argmax(results)\n",
    "            tag = Chatbot.labels[results_index]\n",
    "            if Chatbot.showWorking:\n",
    "                print(\"Highest intent proability:\",results[0][results_index])\n",
    "                print(\"Intent of sentence : \",tag)\n",
    "            responses = Chatbot.responseDictionary[tag]\n",
    "            print(\"chatbot:\",random.choice(responses))\n",
    "            \n",
    "def remove_stop_words_get_lemmas(sentence):\n",
    "        #doc = Chatbot.spacy_nlp(sentence)\n",
    "        #lemmaList = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        s_words = nltk.word_tokenize(sentence)\n",
    "        s_words = [Chatbot.stemmer.stem(word.lower()) for word in s_words]\n",
    "        if Chatbot.showWorking:\n",
    "            #print('Lemma :',lemmaList)\n",
    "            print('Stems :',s_words)\n",
    "        return s_words\n",
    "\n",
    "def bag_of_words(sentence):\n",
    "    bags = []\n",
    "    #creating hot vector\n",
    "    bag = [0 for _ in range(len(Chatbot.vocabulary))] \n",
    "    s_words = remove_stop_words_get_lemmas(sentence)\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(Chatbot.vocabulary):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "                \n",
    "    if Chatbot.showWorking:\n",
    "        print('Vector Representation of input sentence is:',bag)\n",
    "            \n",
    "    bags.append(bag)\n",
    "    bags = numpy.array(bags)\n",
    "    return bags\n",
    "        \n",
    "def main():\n",
    "    chatbot = Chatbot(True)\n",
    "    chatbot.chat()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from importlib import reload \n",
    "reload(spacy)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "while (True):\n",
    "\n",
    "    doc = input(\"Enter :\")\n",
    "\n",
    "    if doc=='quit':\n",
    "        break\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    \n",
    "    doc = spacy_nlp(doc)\n",
    "    customize_stop_words = [\n",
    "        '.', '?','!'\n",
    "    ]\n",
    "    \n",
    "    for w in customize_stop_words:\n",
    "        spacy_nlp.vocab[w].is_stop = True\n",
    "\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "print(spacy.lang.en.stop_words.STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bags = []\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    #print('tokenizer:',s_words)\n",
    "    \n",
    "    print('lemmarizer:',get_lemmas(s))\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    bags.append(bag)\n",
    "    bags = numpy.array(bags)\n",
    "    #return numpy.array(bag)\n",
    "    return bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "    \n",
    "with open ('responseDictionary', 'rb') as fp:\n",
    "    itemlist2 = pickle.load(fp)\n",
    "    \n",
    "print(itemlist2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "#from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "sentence = 'what are you'\n",
    "s_words = nltk.word_tokenize(sentence)\n",
    "s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "print(s_words)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'asds': 'Geeks', 'b': 'For', 'c': 'geeks'} \n",
    "\n",
    "def getList(dict): \n",
    "      \n",
    "    return [*dict] \n",
    "\n",
    "print(dict['dhjjh']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
