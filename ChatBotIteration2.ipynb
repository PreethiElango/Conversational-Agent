{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to see internal processing steps:\n",
      "Enter you choice : yes\n",
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
      "Start talking with the bot (type quit to stop)!\n",
      "You: weather\n",
      "Stems : ['weather']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Highest intent proability: 0.97170275\n",
      "Intent of sentence :  Weather\n",
      "chatbot: Layer up, it is -2 degree celsius\n",
      "You: see you\n",
      "Stems : ['see', 'you']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Highest intent proability: 0.93429035\n",
      "Intent of sentence :  quit\n",
      "chatbot: If you like me please give me good grades\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import h5py\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import spacy\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "class Chatbot:\n",
    "\n",
    "    showWorking = False\n",
    "    train_again=False\n",
    "    \n",
    "    vocabulary=[]\n",
    "    labels=[]\n",
    "    responseDictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    def __init__(self,train_again=False):\n",
    "        Chatbot.train_again = train_again\n",
    "        \n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    customize_stop_words = ['.', '?','!']\n",
    "    for w in customize_stop_words:\n",
    "        spacy_nlp.vocab[w].is_stop = True\n",
    "\n",
    "    def get_model(self):\n",
    "\n",
    "        if not Chatbot.train_again:\n",
    "            with open ('LabelList.json') as fl:\n",
    "                Chatbot.labels = json.load(fl)\n",
    "            \n",
    "            with open ('VocabularyList.json') as fv:\n",
    "                Chatbot.vocabulary = json.load(fv)\n",
    "                \n",
    "            with open ('responseDictionary.json') as fr:\n",
    "                Chatbot.responseDictionary = json.load(fr)\n",
    "                \n",
    "            model= tf.keras.models.load_model(\"chatBotold.h5\")\n",
    "        else:\n",
    "            with open('CommonIntentions.json') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            docs_x = []\n",
    "            docs_y = []\n",
    "\n",
    "            for intent in data['intents']:\n",
    "                \n",
    "                Chatbot.responseDictionary[intent['tag']] = []\n",
    "                responseList=[]\n",
    "                for response in intent['responses']:\n",
    "                    responseList.append(response)\n",
    "                    \n",
    "                Chatbot.responseDictionary[intent['tag']] = responseList\n",
    "                \n",
    "                for pattern in intent['patterns']:\n",
    "                    wrds = remove_stop_words_get_lemmas(pattern)\n",
    "                    if len(wrds)>0:\n",
    "                        Chatbot.vocabulary.extend(wrds)\n",
    "                        docs_x.append(wrds)\n",
    "                        docs_y.append(intent['tag'])               \n",
    "\n",
    "                if intent['tag'] not in Chatbot.labels:\n",
    "                    Chatbot.labels.append(intent['tag'])\n",
    "                    \n",
    "            with open('responseDictionary.json', 'w') as fp:\n",
    "                json.dump(Chatbot.responseDictionary, fp)\n",
    "                \n",
    "            Chatbot.vocabulary = sorted(list(set(Chatbot.vocabulary)))\n",
    "            Chatbot.labels = sorted(Chatbot.labels)\n",
    "            training = []\n",
    "            output = []\n",
    "            out_empty = [0 for _ in range(len(Chatbot.labels))]\n",
    "\n",
    "            for x, doc in enumerate(docs_x):\n",
    "                bag = []\n",
    "\n",
    "                for w in Chatbot.vocabulary:\n",
    "                    if w in doc:\n",
    "                        bag.append(1)\n",
    "                    else:\n",
    "                        bag.append(0)\n",
    "\n",
    "                output_row = out_empty[:]\n",
    "                output_row[Chatbot.labels.index(docs_y[x])] = 1\n",
    "\n",
    "                training.append(bag)\n",
    "                output.append(output_row)\n",
    "\n",
    "            training = numpy.array(training)\n",
    "            output = numpy.array(output)\n",
    "\n",
    "\n",
    "            model = tf.keras.Sequential()\n",
    "            for layer in range(3):\n",
    "            # Adds a densely-connected layer with specified units to the model:\n",
    "                model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "\n",
    "        # Add a sigmoid layer with 1 output units:\n",
    "            model.add(tf.keras.layers.Dense(len(output[0]), activation='softmax'))\n",
    "            model.compile(optimizer='adam',loss='categorical_crossentropy',metric=['accuracy'])\n",
    "            model.fit(training, output, epochs=300)\n",
    "            model.save(\"chatBotold.h5\")\n",
    "            \n",
    "            with open('LabelList.json', 'w') as fl:\n",
    "                json.dump(Chatbot.labels, fl)\n",
    "            with open('VocabularyList.json', 'w') as fv:\n",
    "                json.dump(Chatbot.vocabulary, fv)\n",
    "        return model\n",
    "\n",
    "    def get_yes_list(self):\n",
    "        return ['yes','sure','ok','okay','for sure','sure','totally','yep','ya','yeah','yup','certainly',\n",
    "                         'definitely','of course','gladly','absolutely','indeed']\n",
    "    \n",
    "    def chat(self):\n",
    "        \n",
    "        print(\"Do you want to see internal processing steps:\")\n",
    "        choice = input(\"Enter you choice : \")\n",
    "        if choice.lower() in self.get_yes_list():\n",
    "            Chatbot.showWorking=True\n",
    "        model = self.get_model()\n",
    "        print(\"Start talking with the bot (type quit to stop)!\")\n",
    "        while True:\n",
    "            inp = input(\"You: \")\n",
    "            if inp.lower() == \"quit\":\n",
    "                break\n",
    "            results = model.predict(bag_of_words(inp))\n",
    "            results_index = numpy.argmax(results)\n",
    "            tag = Chatbot.labels[results_index]\n",
    "            if Chatbot.showWorking:\n",
    "                print(\"Highest intent proability:\",results[0][results_index])\n",
    "                print(\"Intent of sentence : \",tag)\n",
    "            responses = Chatbot.responseDictionary[tag]\n",
    "            print(\"chatbot:\",random.choice(responses))\n",
    "            if tag == \"quit\":\n",
    "                break\n",
    "            \n",
    "                        \n",
    "            \n",
    "def remove_stop_words_get_lemmas(sentence):\n",
    "        #doc = Chatbot.spacy_nlp(sentence)\n",
    "        #lemmaList = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        s_words = nltk.word_tokenize(sentence)\n",
    "        s_words = [Chatbot.stemmer.stem(word.lower()) for word in s_words]\n",
    "        if Chatbot.showWorking:\n",
    "            #print('Lemma :',lemmaList)\n",
    "            print('Stems :',s_words)\n",
    "        return s_words\n",
    "\n",
    "def bag_of_words(sentence):\n",
    "    bags = []\n",
    "    #creating hot vector\n",
    "    bag = [0 for _ in range(len(Chatbot.vocabulary))] \n",
    "    s_words = remove_stop_words_get_lemmas(sentence)\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(Chatbot.vocabulary):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "                \n",
    "    if Chatbot.showWorking:\n",
    "        print('Vector Representation of input sentence is:',bag)\n",
    "            \n",
    "    bags.append(bag)\n",
    "    bags = numpy.array(bags)\n",
    "    return bags\n",
    "        \n",
    "def main():\n",
    "    chatbot = Chatbot(False)\n",
    "    chatbot.chat()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from importlib import reload \n",
    "reload(spacy)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "while (True):\n",
    "\n",
    "    doc = input(\"Enter :\")\n",
    "\n",
    "    if doc=='quit':\n",
    "        break\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    \n",
    "    doc = spacy_nlp(doc)\n",
    "    customize_stop_words = [\n",
    "        '.', '?','!'\n",
    "    ]\n",
    "    \n",
    "    for w in customize_stop_words:\n",
    "        spacy_nlp.vocab[w].is_stop = True\n",
    "\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "print(spacy.lang.en.stop_words.STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bags = []\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    #print('tokenizer:',s_words)\n",
    "    \n",
    "    print('lemmarizer:',get_lemmas(s))\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    bags.append(bag)\n",
    "    bags = numpy.array(bags)\n",
    "    #return numpy.array(bag)\n",
    "    return bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "    \n",
    "with open ('responseDictionary', 'rb') as fp:\n",
    "    itemlist2 = pickle.load(fp)\n",
    "    \n",
    "print(itemlist2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "#from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "sentence = 'what are you'\n",
    "s_words = nltk.word_tokenize(sentence)\n",
    "s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "print(s_words)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'asds': 'Geeks', 'b': 'For', 'c': 'geeks'} \n",
    "\n",
    "def getList(dict): \n",
    "      \n",
    "    return [*dict] \n",
    "\n",
    "print(dict['dhjjh']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
