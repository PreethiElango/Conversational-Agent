{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to see internal processing steps:\n",
      "Enter you choice : yes\n",
      "Stems : ['tell', 'me', 'some', 'car', 'which', 'are', 'in', 'my', 'budget']\n",
      "Stems : ['cheap', 'car']\n",
      "Stems : ['cheapest', 'car']\n",
      "Stems : ['afford', 'car']\n",
      "Stems : ['low', 'budget', 'car']\n",
      "Stems : ['lower', 'end', 'car']\n",
      "Stems : ['low', 'maintainc', 'car']\n",
      "Stems : ['low', 'cost', 'car']\n",
      "Stems : ['econom', 'car']\n",
      "Stems : ['which', 'car', 'is', 'slowest', '?']\n",
      "Stems : ['which', 'is', 'slowest', 'car', '?']\n",
      "Stems : ['slow', 'car']\n",
      "Stems : ['afford', 'luxuri', 'car']\n",
      "Stems : ['top', 'car']\n",
      "Stems : ['high', 'class', 'car']\n",
      "Stems : ['high', 'end', 'car']\n",
      "Stems : ['expens', 'car']\n",
      "Stems : ['which', 'car', 'is', 'fastest', '?']\n",
      "Stems : ['which', 'is', 'fastest', 'car']\n",
      "Stems : ['fast', 'car']\n",
      "Stems : ['what', 'car', 'i', 'should', 'buy']\n",
      "Stems : ['which', 'car', 'to', 'buy']\n",
      "Stems : ['trend', 'car']\n",
      "Stems : ['differ', 'compani', 'of', 'car']\n",
      "Stems : ['which', 'is', 'the', 'latest', 'car']\n",
      "Stems : ['tell', 'me', 'some', 'mileag', 'car']\n",
      "Stems : ['what', 'car', 'i', 'should', 'buy']\n",
      "Stems : ['tell', 'me', 'some', 'good', 'car', 'name']\n",
      "Stems : ['tell', 'me', 'some', 'car', 'compani', 'name']\n",
      "Stems : ['what', 'is', 'a', 'good', 'car', 'for', 'student']\n",
      "Stems : ['good', 'car', 'for', 'elder']\n",
      "Stems : ['good', 'car', 'for', 'women']\n",
      "Stems : ['most', 'famou', 'car']\n",
      "Stems : ['trend', 'car']\n",
      "Stems : ['how', 'to', 'buy', 'a', 'car']\n",
      "Stems : ['buy', 'car']\n",
      "Stems : ['car', 'specif']\n",
      "Stems : ['car', 'featur']\n",
      "Stems : ['sport', 'car', 'under']\n",
      "Stems : ['sexi', 'car']\n",
      "Stems : ['race', 'car']\n",
      "Stems : ['car', 'for', 'boy']\n",
      "Stems : ['how', 'much', 'mileag', 'car', 'give']\n",
      "Stems : ['mileag', 'of', 'a', 'car']\n",
      "Stems : ['mile', 'per', 'gallon']\n",
      "Stems : ['mpg']\n",
      "Stems : ['can', 'i', 'switch', 'to', 'synthet', 'oil']\n",
      "Stems : ['do', 'you', 'have', 'a', 'car', 'that', 'fit', 'my', 'need']\n",
      "Stems : ['doe', 'car', 'break']\n",
      "Stems : ['doe', 'car', 'have', 'headlight']\n",
      "Stems : ['should', 'i', 'buy', 'a', 'car']\n",
      "Stems : ['do', 'you', 'have', 'a', 'car']\n",
      "Stems : ['can', 'boy', 'drive', 'car']\n",
      "Stems : ['can', 'men', 'drive', 'car']\n",
      "Stems : ['can', 'women', 'drive', 'car']\n",
      "Stems : ['doe', 'car', 'have', 'wheel']\n",
      "Stems : ['doe', 'car', 'have', 'engin']\n",
      "Stems : ['doe', 'run', 'on', 'road']\n",
      "Stems : ['doe', 'car', 'produc', 'smoke']\n",
      "Stems : ['can', '4', 'peopl', 'sit', 'in', 'a', 'car']\n",
      "Stems : ['advantag', 'of', 'car']\n",
      "Stems : ['pro', 'of', 'car']\n",
      "Stems : ['use', 'of', 'car']\n",
      "Stems : ['who', 'use', 'car']\n",
      "Stems : ['disadvantag', 'of', 'car']\n",
      "Stems : ['common', 'issu', 'which', 'car']\n",
      "Stems : ['con', 'of', 'car']\n",
      "Stems : ['use', 'of', 'car']\n",
      "Stems : ['ugli', 'car']\n",
      "Stems : ['what', 'are', 'some', 'common', 'repair']\n",
      "Stems : ['how', 'car', 'work']\n",
      "Stems : ['how', 'car', 'oper']\n",
      "Stems : ['work', 'principl', 'of', 'car']\n",
      "Stems : ['what', 'it', 'take', 'to', 'run', 'a', 'car']\n",
      "Stems : ['compon', 'of', 'car']\n",
      "Stems : ['part', 'of', 'the', 'car']\n",
      "Stems : ['how', 'mani', 'gear', 'car', 'can', 'have']\n",
      "Stems : ['how', 'mani', 'door', 'car', 'can', 'have']\n",
      "Stems : ['how', 'mani', 'peopl', 'can', 'sit', 'in', 'a', 'car']\n",
      "Stems : ['how', 'to', 'chang', 'gear']\n",
      "Stems : ['how', 'to', 'appli', 'revers', 'gear']\n",
      "Stems : ['switch', 'gear']\n",
      "Stems : ['how', 'to', 'revers', 'a', 'car']\n",
      "Stems : ['how', 'to', 'park', 'a', 'car']\n",
      "Stems : ['how', 'mani', 'headlight', 'car', 'have']\n",
      "Stems : ['how', 'mani', 'tail', 'light', 'car', 'can', 'have']\n",
      "Stems : ['how', 'mani', 'indic', 'car', 'have']\n",
      "Stems : ['what', 'is', 'the', 'mileag', 'of', 'your', 'car']\n",
      "Stems : ['averag', 'fuel', 'consumpt']\n",
      "Stems : ['how', 'to', 'start', 'a', 'car']\n",
      "Stems : ['how', 'to', 'stop', 'a', 'car']\n",
      "Stems : ['how', 'to', 'drive', 'a', 'car']\n",
      "Stems : ['tell', 'me', 'about', 'gear']\n",
      "Stems : ['kind', 'of', 'gear', 'car', 'can', 'have']\n",
      "Stems : ['how', 'frequent', 'should', 'serv', 'of', 'car', 'be', 'done']\n",
      "Stems : ['pollut', 'check', 'up']\n",
      "Stems : ['how', 'often', 'should', 'i', 'check', 'my', 'tire', 'pressur']\n",
      "Stems : ['which', 'kind', 'of', 'fuel', 'car', 'use']\n",
      "Stems : ['what', 'provid', 'energi', 'to', 'drive', 'car']\n",
      "Stems : ['how', 'car', 'oper']\n",
      "Stems : ['what', 'kind', 'of', 'transmiss', 'car', 'have']\n",
      "Stems : ['which', 'car', 'is', 'better', 'manual', 'or', 'automat']\n",
      "Stems : ['differnt', 'mode', 'of', 'transmiss']\n",
      "Stems : ['which', 'transmiss', 'type', 'is', 'good']\n",
      "Stems : ['color', 'car', 'have']\n",
      "Stems : ['differ', 'color', 'of', 'car']\n",
      "Stems : ['shade', 'of', 'car']\n",
      "Stems : ['how', 'much', 'we', 'have', 'to', 'invest', 'in', 'repair', 'of', 'a', 'car']\n",
      "Stems : ['maximum', 'speed', 'of', 'car']\n",
      "Stems : ['how', 'can', 'i', 'save', 'money', 'on', 'ga']\n",
      "Stems : ['how', 'much', 'a', 'car', 'cost']\n",
      "Stems : ['price', 'of', 'car']\n",
      "Stems : ['are', 'these', 'autonom', 'vehicl', ',', 'good', 'driver']\n",
      "Stems : ['thing', 'to', 'consid', 'while', 'drive', 'car']\n",
      "Stems : ['tip', 'for', 'first', 'time', 'driver']\n",
      "Stems : ['rule', 'for', 'drive']\n",
      "Stems : ['which', 'is', 'a', 'good', 'four-wheel', 'drive', 'or', 'two', 'drive']\n",
      "Stems : ['what', 'is', 'a', 'four-wheel', 'drive']\n",
      "Stems : ['what', 'is', 'two-wheel', 'drive']\n",
      "Stems : ['where', 'can', 'we', 'drive', 'the', 'car']\n",
      "Stems : ['what', 'happen', 'if', 'someon', 'hit', 'the', 'car']\n",
      "Stems : ['what', 'other', 'amen', 'car', 'have']\n",
      "Stems : ['featur', 'of', 'car']\n",
      "Stems : ['how', 'to', 'acceler', 'a', 'car']\n",
      "Stems : []\n",
      "Stems : ['who', 'invent', 'car']\n",
      "Stems : ['when', 'wa', 'car', 'invent']\n",
      "Stems : ['what', 'is', 'car']\n",
      "Stems : ['fact', 'about', 'car']\n",
      "Stems : ['car']\n",
      "Stems : ['car']\n",
      "Stems : ['how', 'to', 'acceler', 'a', 'car']\n",
      "Stems : ['how', 'to', 'remov', 'the', 'seat', 'belt']\n",
      "Stems : ['do', 'we', 'have', 'to', 'wear', 'seat', 'belt']\n",
      "Stems : ['brake']\n",
      "Stems : ['how', 'to', 'appli', 'the', 'brake']\n",
      "Stems : ['how', 'to', 'use', 'wiper', 'blade']\n",
      "Stems : ['how', 'to', 'turn', 'ac', 'on']\n",
      "Stems : ['turn', 'ac', 'off']\n",
      "Stems : ['ford']\n",
      "Stems : ['tesla']\n",
      "Stems : ['bmw']\n",
      "Stems : ['audi']\n",
      "Stems : ['jeep']\n",
      "Stems : ['ferrari']\n",
      "Stems : ['mustang']\n",
      "Stems : ['honda']\n",
      "Stems : ['tata']\n",
      "Stems : ['bugatta']\n",
      "Stems : ['dodg']\n",
      "Stems : ['toyata']\n",
      "Stems : ['volvo']\n",
      "Stems : ['nissan']\n",
      "Stems : ['buick']\n",
      "Stems : ['chevrolet']\n",
      "Stems : ['chrysler']\n",
      "Stems : ['hyundai']\n",
      "Stems : ['what', 'are', 'variou', 'kind', 'of', 'car']\n",
      "Stems : ['type', 'of', 'car']\n",
      "Stems : ['car', 'list']\n",
      "Stems : ['hi']\n",
      "Stems : ['good', 'day']\n",
      "Stems : ['hello', '!']\n",
      "Stems : ['hey']\n",
      "Stems : ['good', 'morn']\n",
      "Stems : ['gud', 'mrng']\n",
      "Stems : ['hola']\n",
      "Stems : ['who', 'made', 'you']\n",
      "Stems : ['who', 'is', 'your', 'owner']\n",
      "Stems : ['who', 'are', 'your', 'maker']\n",
      "Stems : ['who', 'is', 'your', 'creater']\n",
      "Stems : ['who', 'creat', 'you']\n",
      "Stems : ['manufactur']\n",
      "Stems : ['who', 'built', 'you']\n",
      "Stems : ['how', 'are', 'you', 'abl', 'to', 'answer']\n",
      "Stems : ['how', 'you', 'work']\n",
      "Stems : ['explain', 'your', 'work', 'principl']\n",
      "Stems : ['mechan', 'of', 'work']\n",
      "Stems : ['principl', 'of', 'work']\n",
      "Stems : ['mode', 'of', 'exceut']\n",
      "Stems : ['way', 'of', 'answer']\n",
      "Stems : ['what', 'do', 'you', 'do']\n",
      "Stems : ['how', 'do', 'you', 'talk']\n",
      "Stems : ['how', 'you', 'repli']\n",
      "Stems : ['how', 'you', 'respons']\n",
      "Stems : ['your', 'respons', 'time']\n",
      "Stems : ['how', 'old', 'are', 'you']\n",
      "Stems : ['what', 'is', 'your', 'birth', 'date']\n",
      "Stems : ['when', 'your', 'birthday', 'come']\n",
      "Stems : ['what', 'is', 'your', 'age']\n",
      "Stems : ['age', '?']\n",
      "Stems : ['date', 'of', 'birth']\n",
      "Stems : ['are', 'you', 'talent']\n",
      "Stems : ['how', 'talent', 'you', 'are']\n",
      "Stems : ['what', 'question', 'you', 'can', 'answer']\n",
      "Stems : ['you', 'are', 'expert', 'in']\n",
      "Stems : ['you', 'domain']\n",
      "Stems : ['your', 'context']\n",
      "Stems : ['you', 'are', 'trani', 'in']\n",
      "Stems : ['how', 'to', 'make', 'chatbot']\n",
      "Stems : ['what', 'are', 'your', 'part']\n",
      "Stems : ['main', 'compon']\n",
      "Stems : ['constitu']\n",
      "Stems : ['in', 'which', 'languag', 'you', 'can', 'talk']\n",
      "Stems : ['how', 'you', 'talk']\n",
      "Stems : ['how', 'you', 'speak']\n",
      "Stems : ['you', 'speak', 'in']\n",
      "Stems : ['which', 'languag', 'you', 'use']\n",
      "Stems : ['medium', 'of', 'commun']\n",
      "Stems : ['mode', 'of', 'interact']\n",
      "Stems : ['way', 'of', 'talk']\n",
      "Stems : ['can', 'you', 'speak']\n",
      "Stems : ['what', 'is', 'your', 'name', '?']\n",
      "Stems : ['name', '?']\n",
      "Stems : ['how', 'to', 'call', 'you', '?']\n",
      "Stems : ['what', 'to', 'call', 'you']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems : ['hey', 'i', 'am']\n",
      "Stems : ['your', 'name', '?']\n",
      "Stems : ['who', 'are', 'you', '?']\n",
      "Stems : ['where', 'to', 'get', 'inform', 'about', 'chatbot']\n",
      "Stems : ['what', 'is', 'chatbot']\n",
      "Stems : ['defin', 'chatbot']\n",
      "Stems : ['creat', 'chatbot']\n",
      "Stems : ['who', 'invent', 'chatbot']\n",
      "Stems : ['first', 'chatbot', 'come', 'in']\n",
      "Stems : ['chatbot']\n",
      "Stems : ['chat']\n",
      "Stems : ['bot']\n",
      "Stems : ['chatbot', 'exit', 'sinc']\n",
      "Stems : ['name', 'of', 'first', 'chatbot']\n",
      "Stems : ['histori', 'of', 'chatbot']\n",
      "Stems : ['use', 'of', 'chatbot']\n",
      "Stems : ['advantag', 'of', 'chatbot']\n",
      "Stems : ['applic', 'of', 'chatbot']\n",
      "Stems : ['pratic', 'use', 'of', 'chatbot']\n",
      "Stems : ['whi', 'should', 'i', 'use', 'you']\n",
      "Stems : ['whi', 'to', 'use', 'you']\n",
      "Stems : ['what', 'i', 'will', 'get', 'out', 'of']\n",
      "Stems : ['pro', 'of', 'chatbot']\n",
      "Stems : ['tell', 'me', 'someth', 'bad', 'about', 'you']\n",
      "Stems : ['case', 'where', 'chatbot', 'fail']\n",
      "Stems : ['chatbot']\n",
      "Stems : ['see', 'you', 'later']\n",
      "Stems : ['bye']\n",
      "Stems : ['see', 'you', 'soon']\n",
      "Stems : ['talk', 'to', 'you', 'later']\n",
      "Stems : ['catch', 'up', 'later']\n",
      "Stems : ['will', 'meet', 'again']\n",
      "Stems : ['good', 'bye']\n",
      "Stems : ['will', 'talk', 'later']\n",
      "Stems : ['nice', 'meet', 'talk', 'with', 'you']\n",
      "Stems : ['will', 'catch', 'you', 'later']\n",
      "Stems : ['quit']\n",
      "Stems : ['byye']\n",
      "Stems : ['catch', 'you', 'later']\n",
      "Stems : ['i', \"'d\", 'like', 'to', 'talk', 'about', 'weather']\n",
      "Stems : ['talk', 'about', 'weather']\n",
      "Stems : ['discuss', 'weather']\n",
      "Stems : ['discuss', 'about', 'car']\n",
      "Stems : ['i', \"'d\", 'like', 'to', 'talk', 'about', 'car']\n",
      "Stems : ['talk', 'about', 'car']\n",
      "Stems : ['discuss', 'about', 'chatbot']\n",
      "Stems : ['i', \"'d\", 'like', 'to', 'talk', 'about', 'chatbot']\n",
      "Stems : ['talk', 'about', 'chatbot']\n",
      "Stems : ['what', \"'s\", 'the', 'weather', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'weather', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'weather', 'now', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'weather', 'today', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'weather', 'now', '?']\n",
      "Stems : ['what', 'is', 'the', 'weather', 'predict', 'today', '?']\n",
      "Stems : ['what', 'is', 'the', 'weather', 'predict', 'now', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'weather', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'weather', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'weather', 'now', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'weather', 'today', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'weather', 'now', '?']\n",
      "Stems : ['weather']\n",
      "Stems : ['climat']\n",
      "Stems : ['will', 'it', 'rain', 'today', '?']\n",
      "Stems : ['is', 'it', 'rain', 'outsid', '?']\n",
      "Stems : ['will', 'it', 'rain', 'now', '?']\n",
      "Stems : ['rain', '?']\n",
      "Stems : ['is', 'it', 'predict', 'to', 'rain', '?']\n",
      "Stems : ['whi', 'it', 'is', 'rain', '?']\n",
      "Stems : ['is', 'there', 'predict', 'shower', 'today', '?']\n",
      "Stems : ['will', 'it', 'shower', 'today', '?']\n",
      "Stems : ['can', 'it', 'shower', 'today', '?']\n",
      "Stems : ['can', 'it', 'rain', 'today', '?']\n",
      "Stems : ['do', 'i', 'need', 'to', 'take', 'umbrella', 'today', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'temperatur', 'now', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'temperatur', 'now', '?']\n",
      "Stems : ['what', 'is', 'the', 'weather', 'predict', 'today', '?']\n",
      "Stems : ['what', 'is', 'the', 'temperatur', 'predict', 'now', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'temperatur', 'now', '?']\n",
      "Stems : ['are', 'you', 'awar', 'of', 'the', 'temperatur', 'today', '?']\n",
      "Stems : ['what', \"'s\", 'the', 'temperatur', 'now', '?']\n",
      "Stems : ['will', 'it', 'snow', 'today', '?']\n",
      "Stems : ['is', 'it', 'snow', 'outsid', '?']\n",
      "Stems : ['will', 'it', 'snow', 'now', '?']\n",
      "Stems : ['snow', '?']\n",
      "Stems : ['is', 'it', 'predict', 'to', 'snow', '?']\n",
      "Stems : ['whi', 'it', 'is', 'snow', '?']\n",
      "Stems : ['is', 'there', 'predict', 'snow', 'today', '?']\n",
      "Stems : ['will', 'it', 'snow', 'today', '?']\n",
      "Stems : ['can', 'it', 'snow', 'today', '?']\n",
      "Stems : ['can', 'it', 'snow', 'today', '?']\n",
      "Stems : ['do', 'i', 'need', 'to', 'take', 'jacket', 'today', '?']\n",
      "Stems : ['can', 'i', 'take', 'jacket', 'today', '?']\n",
      "Stems : ['should', 'i', 'take', 'jacket', 'today', '?']\n",
      "Stems : ['should', 'i', 'layer', 'up', 'today', '?']\n",
      "Stems : ['should', 'i', 'layer', 'up', 'now', '?']\n",
      "Stems : ['is', 'it', 'sunni', '?']\n",
      "Stems : ['is', 'it', 'hot', 'today', '?']\n",
      "Stems : ['it', 'seem', 'hot', ',', 'will', 'there', 'be', 'heat', 'stroke', '?']\n",
      "Stems : ['hot', 'now', '?']\n",
      "Stems : ['hot', 'tomorrow', '?']\n",
      "Stems : ['is', 'it', 'cloudi', '?']\n",
      "Stems : ['is', 'it', 'cloudi', 'today', '?']\n",
      "Stems : ['it', 'seem', 'cloudi', ',', 'will', 'there', 'be', 'shower', '?']\n",
      "Stems : ['what', 'is', 'the', 'season', 'now', '?']\n",
      "Stems : ['season']\n",
      "Stems : ['season']\n",
      "Stems : ['is', 'it', 'summer']\n",
      "Stems : ['is', 'it', 'winter']\n",
      "Stems : ['is', 'it', 'fall']\n",
      "Stems : ['when', 'is', 'summer']\n",
      "Stems : ['is', 'there', 'posibl', 'of', 'hurrican', '?']\n",
      "Stems : ['hurrican', '?']\n",
      "Stems : ['will', 'hurrican', 'hit', '?']\n",
      "Stems : ['is', 'there', 'a', 'posibl', 'of', 'snow', 'storm', '?']\n",
      "Stems : ['snowstorm', '?']\n",
      "Stems : ['snow', 'storm', '?']\n",
      "Stems : ['is', 'there', 'a', 'posibl', 'of', 'storm', '?']\n",
      "Stems : ['is', 'there', 'possibl', 'of', 'cyclon', '?']\n",
      "Stems : ['will', 'cyclon', 'hit', 'us', '?']\n",
      "Stems : ['ani', 'possibl', 'of', 'cyclon', 'soon', '?']\n",
      "Stems : ['is', 'ani', 'calam', 'possibl', 'now', '?']\n",
      "Train on 341 samples\n",
      "Epoch 1/300\n",
      "341/341 [==============================] - ETA: 9s - loss: 3.968 - 1s 3ms/sample - loss: 3.9659\n",
      "Epoch 2/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.957 - 0s 155us/sample - loss: 3.9545\n",
      "Epoch 3/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.959 - 0s 126us/sample - loss: 3.9432\n",
      "Epoch 4/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.942 - 0s 120us/sample - loss: 3.9304\n",
      "Epoch 5/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.920 - 0s 147us/sample - loss: 3.9147\n",
      "Epoch 6/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.902 - 0s 132us/sample - loss: 3.8933\n",
      "Epoch 7/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.864 - 0s 117us/sample - loss: 3.8673\n",
      "Epoch 8/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.892 - ETA: 0s - loss: 3.834 - 0s 229us/sample - loss: 3.8333\n",
      "Epoch 9/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.826 - 0s 141us/sample - loss: 3.7916\n",
      "Epoch 10/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.846 - 0s 132us/sample - loss: 3.7398\n",
      "Epoch 11/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.768 - 0s 144us/sample - loss: 3.6802\n",
      "Epoch 12/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.671 - 0s 103us/sample - loss: 3.6165\n",
      "Epoch 13/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.605 - 0s 114us/sample - loss: 3.5529\n",
      "Epoch 14/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.449 - 0s 123us/sample - loss: 3.4851\n",
      "Epoch 15/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.714 - 0s 120us/sample - loss: 3.4198\n",
      "Epoch 16/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.272 - 0s 147us/sample - loss: 3.3534\n",
      "Epoch 17/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.440 - 0s 114us/sample - loss: 3.2882\n",
      "Epoch 18/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.147 - 0s 109us/sample - loss: 3.2202\n",
      "Epoch 19/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 3.027 - 0s 141us/sample - loss: 3.1521\n",
      "Epoch 20/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.949 - 0s 138us/sample - loss: 3.0837\n",
      "Epoch 21/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.674 - 0s 147us/sample - loss: 3.0129\n",
      "Epoch 22/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.817 - 0s 117us/sample - loss: 2.9425\n",
      "Epoch 23/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.931 - 0s 179us/sample - loss: 2.8736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.719 - 0s 132us/sample - loss: 2.8036\n",
      "Epoch 25/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.729 - 0s 109us/sample - loss: 2.7365\n",
      "Epoch 26/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.827 - 0s 132us/sample - loss: 2.6708\n",
      "Epoch 27/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.527 - 0s 138us/sample - loss: 2.6077\n",
      "Epoch 28/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.476 - 0s 129us/sample - loss: 2.5444\n",
      "Epoch 29/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.389 - 0s 150us/sample - loss: 2.4822\n",
      "Epoch 30/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.238 - 0s 161us/sample - loss: 2.4204\n",
      "Epoch 31/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.249 - ETA: 0s - loss: 2.372 - 0s 179us/sample - loss: 2.3594\n",
      "Epoch 32/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.121 - 0s 120us/sample - loss: 2.2999\n",
      "Epoch 33/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.439 - 0s 88us/sample - loss: 2.2395\n",
      "Epoch 34/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.164 - 0s 91us/sample - loss: 2.1784\n",
      "Epoch 35/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.218 - 0s 82us/sample - loss: 2.1196\n",
      "Epoch 36/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.112 - 0s 79us/sample - loss: 2.0607\n",
      "Epoch 37/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 2.262 - 0s 106us/sample - loss: 2.0031\n",
      "Epoch 38/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.976 - 0s 73us/sample - loss: 1.9463\n",
      "Epoch 39/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.961 - 0s 73us/sample - loss: 1.8907\n",
      "Epoch 40/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.471 - 0s 70us/sample - loss: 1.8351\n",
      "Epoch 41/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.664 - 0s 76us/sample - loss: 1.7806\n",
      "Epoch 42/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.805 - 0s 73us/sample - loss: 1.7242\n",
      "Epoch 43/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.669 - 0s 73us/sample - loss: 1.6685\n",
      "Epoch 44/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.419 - 0s 67us/sample - loss: 1.6157\n",
      "Epoch 45/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.603 - 0s 106us/sample - loss: 1.5630\n",
      "Epoch 46/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.667 - 0s 82us/sample - loss: 1.5088\n",
      "Epoch 47/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.392 - 0s 79us/sample - loss: 1.4591\n",
      "Epoch 48/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.312 - 0s 53us/sample - loss: 1.4101\n",
      "Epoch 49/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.397 - 0s 50us/sample - loss: 1.3610\n",
      "Epoch 50/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.467 - 0s 53us/sample - loss: 1.3129\n",
      "Epoch 51/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.281 - 0s 53us/sample - loss: 1.2671\n",
      "Epoch 52/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.236 - 0s 88us/sample - loss: 1.2214\n",
      "Epoch 53/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.148 - 0s 94us/sample - loss: 1.1786\n",
      "Epoch 54/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.209 - 0s 85us/sample - loss: 1.1349\n",
      "Epoch 55/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.088 - 0s 94us/sample - loss: 1.0919\n",
      "Epoch 56/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.919 - 0s 85us/sample - loss: 1.0530\n",
      "Epoch 57/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 1.274 - 0s 76us/sample - loss: 1.0120\n",
      "Epoch 58/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.821 - 0s 114us/sample - loss: 0.9773\n",
      "Epoch 59/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.896 - 0s 85us/sample - loss: 0.9384\n",
      "Epoch 60/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.785 - 0s 79us/sample - loss: 0.9038\n",
      "Epoch 61/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.833 - 0s 85us/sample - loss: 0.8700\n",
      "Epoch 62/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.913 - 0s 79us/sample - loss: 0.8375\n",
      "Epoch 63/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.595 - 0s 85us/sample - loss: 0.8048\n",
      "Epoch 64/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.886 - 0s 79us/sample - loss: 0.7743\n",
      "Epoch 65/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.660 - 0s 79us/sample - loss: 0.7447\n",
      "Epoch 66/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.767 - 0s 76us/sample - loss: 0.7148\n",
      "Epoch 67/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.971 - 0s 82us/sample - loss: 0.6869\n",
      "Epoch 68/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.539 - 0s 73us/sample - loss: 0.6618\n",
      "Epoch 69/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.676 - 0s 114us/sample - loss: 0.6347\n",
      "Epoch 70/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.771 - 0s 111us/sample - loss: 0.6089\n",
      "Epoch 71/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.641 - 0s 88us/sample - loss: 0.5861\n",
      "Epoch 72/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.598 - 0s 79us/sample - loss: 0.5603\n",
      "Epoch 73/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.468 - 0s 82us/sample - loss: 0.5403\n",
      "Epoch 74/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.729 - 0s 85us/sample - loss: 0.5193\n",
      "Epoch 75/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.458 - ETA: 0s - loss: 0.488 - 0s 235us/sample - loss: 0.4998\n",
      "Epoch 76/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.386 - 0s 135us/sample - loss: 0.4806\n",
      "Epoch 77/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.461 - 0s 85us/sample - loss: 0.4639\n",
      "Epoch 78/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.504 - 0s 111us/sample - loss: 0.4464\n",
      "Epoch 79/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.423 - 0s 88us/sample - loss: 0.4301\n",
      "Epoch 80/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.370 - 0s 79us/sample - loss: 0.4161\n",
      "Epoch 81/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.409 - 0s 120us/sample - loss: 0.4019\n",
      "Epoch 82/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.395 - 0s 88us/sample - loss: 0.3877\n",
      "Epoch 83/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.525 - 0s 79us/sample - loss: 0.3761\n",
      "Epoch 84/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.313 - 0s 114us/sample - loss: 0.3626\n",
      "Epoch 85/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.291 - 0s 76us/sample - loss: 0.3514\n",
      "Epoch 86/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.251 - 0s 117us/sample - loss: 0.3376\n",
      "Epoch 87/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.240 - 0s 94us/sample - loss: 0.3290\n",
      "Epoch 88/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.269 - 0s 106us/sample - loss: 0.3190\n",
      "Epoch 89/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.362 - 0s 103us/sample - loss: 0.3089\n",
      "Epoch 90/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.378 - 0s 106us/sample - loss: 0.2995\n",
      "Epoch 91/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.197 - 0s 109us/sample - loss: 0.2918\n",
      "Epoch 92/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.298 - 0s 117us/sample - loss: 0.2818\n",
      "Epoch 93/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.332 - 0s 76us/sample - loss: 0.2747\n",
      "Epoch 94/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.281 - 0s 117us/sample - loss: 0.2671\n",
      "Epoch 95/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.226 - 0s 120us/sample - loss: 0.2594\n",
      "Epoch 96/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.245 - 0s 132us/sample - loss: 0.2535\n",
      "Epoch 97/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.347 - 0s 82us/sample - loss: 0.2459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.239 - 0s 117us/sample - loss: 0.2399\n",
      "Epoch 99/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.407 - 0s 106us/sample - loss: 0.2336\n",
      "Epoch 100/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.134 - 0s 111us/sample - loss: 0.2275\n",
      "Epoch 101/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.216 - 0s 109us/sample - loss: 0.2208\n",
      "Epoch 102/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.209 - 0s 106us/sample - loss: 0.2169\n",
      "Epoch 103/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.271 - 0s 109us/sample - loss: 0.2108\n",
      "Epoch 104/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.114 - 0s 126us/sample - loss: 0.2059\n",
      "Epoch 105/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.172 - 0s 114us/sample - loss: 0.2015\n",
      "Epoch 106/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.214 - 0s 114us/sample - loss: 0.1968\n",
      "Epoch 107/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.262 - 0s 138us/sample - loss: 0.1936\n",
      "Epoch 108/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.139 - 0s 82us/sample - loss: 0.1870\n",
      "Epoch 109/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.227 - 0s 111us/sample - loss: 0.1830\n",
      "Epoch 110/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.218 - 0s 114us/sample - loss: 0.1804\n",
      "Epoch 111/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.209 - 0s 129us/sample - loss: 0.1791\n",
      "Epoch 112/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.197 - 0s 100us/sample - loss: 0.1725\n",
      "Epoch 113/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.136 - 0s 111us/sample - loss: 0.1685\n",
      "Epoch 114/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.310 - 0s 117us/sample - loss: 0.1667\n",
      "Epoch 115/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.223 - 0s 126us/sample - loss: 0.1620\n",
      "Epoch 116/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.096 - 0s 138us/sample - loss: 0.1590\n",
      "Epoch 117/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.137 - 0s 132us/sample - loss: 0.1564\n",
      "Epoch 118/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.126 - 0s 111us/sample - loss: 0.1528\n",
      "Epoch 119/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.191 - 0s 114us/sample - loss: 0.1476\n",
      "Epoch 120/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.097 - 0s 132us/sample - loss: 0.1463\n",
      "Epoch 121/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.226 - 0s 117us/sample - loss: 0.1431\n",
      "Epoch 122/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.067 - 0s 135us/sample - loss: 0.1409\n",
      "Epoch 123/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.096 - 0s 111us/sample - loss: 0.1393\n",
      "Epoch 124/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.069 - 0s 106us/sample - loss: 0.1357\n",
      "Epoch 125/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.124 - 0s 138us/sample - loss: 0.1321\n",
      "Epoch 126/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.131 - 0s 103us/sample - loss: 0.1302\n",
      "Epoch 127/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.115 - 0s 120us/sample - loss: 0.1294\n",
      "Epoch 128/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.089 - 0s 135us/sample - loss: 0.1265\n",
      "Epoch 129/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.074 - 0s 114us/sample - loss: 0.1232\n",
      "Epoch 130/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.217 - 0s 144us/sample - loss: 0.1226\n",
      "Epoch 131/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.259 - 0s 85us/sample - loss: 0.1209\n",
      "Epoch 132/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.075 - 0s 114us/sample - loss: 0.1167\n",
      "Epoch 133/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.114 - 0s 114us/sample - loss: 0.1164\n",
      "Epoch 134/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.050 - 0s 138us/sample - loss: 0.1146\n",
      "Epoch 135/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.059 - 0s 120us/sample - loss: 0.1115\n",
      "Epoch 136/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.051 - 0s 106us/sample - loss: 0.1101\n",
      "Epoch 137/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.171 - 0s 109us/sample - loss: 0.1078\n",
      "Epoch 138/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.068 - 0s 117us/sample - loss: 0.1062\n",
      "Epoch 139/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.101 - 0s 114us/sample - loss: 0.1044\n",
      "Epoch 140/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.075 - 0s 117us/sample - loss: 0.1031\n",
      "Epoch 141/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.132 - 0s 120us/sample - loss: 0.1019\n",
      "Epoch 142/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.064 - 0s 138us/sample - loss: 0.1003\n",
      "Epoch 143/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.097 - 0s 196us/sample - loss: 0.0986\n",
      "Epoch 144/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.096 - 0s 120us/sample - loss: 0.0987\n",
      "Epoch 145/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.066 - 0s 153us/sample - loss: 0.0967\n",
      "Epoch 146/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.236 - 0s 150us/sample - loss: 0.0944\n",
      "Epoch 147/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.045 - 0s 117us/sample - loss: 0.0922\n",
      "Epoch 148/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.125 - 0s 120us/sample - loss: 0.0916\n",
      "Epoch 149/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.129 - 0s 106us/sample - loss: 0.0896\n",
      "Epoch 150/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.086 - 0s 136us/sample - loss: 0.0898\n",
      "Epoch 151/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.090 - 0s 249us/sample - loss: 0.0878\n",
      "Epoch 152/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.137 - 0s 161us/sample - loss: 0.0863\n",
      "Epoch 153/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.056 - 0s 73us/sample - loss: 0.0849\n",
      "Epoch 154/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.080 - 0s 73us/sample - loss: 0.0853\n",
      "Epoch 155/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.164 - 0s 73us/sample - loss: 0.0844\n",
      "Epoch 156/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.033 - 0s 81us/sample - loss: 0.0820\n",
      "Epoch 157/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.173 - 0s 95us/sample - loss: 0.0818\n",
      "Epoch 158/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.129 - 0s 81us/sample - loss: 0.0807\n",
      "Epoch 159/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.113 - 0s 73us/sample - loss: 0.0786\n",
      "Epoch 160/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.035 - 0s 73us/sample - loss: 0.0793\n",
      "Epoch 161/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.030 - 0s 81us/sample - loss: 0.0760\n",
      "Epoch 162/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.082 - 0s 66us/sample - loss: 0.0771\n",
      "Epoch 163/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.018 - 0s 88us/sample - loss: 0.0746\n",
      "Epoch 164/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.068 - 0s 81us/sample - loss: 0.0752\n",
      "Epoch 165/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.086 - 0s 73us/sample - loss: 0.0744\n",
      "Epoch 166/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.059 - 0s 73us/sample - loss: 0.0721\n",
      "Epoch 167/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.100 - 0s 66us/sample - loss: 0.0716\n",
      "Epoch 168/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.048 - 0s 73us/sample - loss: 0.0703\n",
      "Epoch 169/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.056 - 0s 66us/sample - loss: 0.0695\n",
      "Epoch 170/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.164 - 0s 66us/sample - loss: 0.0699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.055 - 0s 66us/sample - loss: 0.0682\n",
      "Epoch 172/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.102 - 0s 66us/sample - loss: 0.0663\n",
      "Epoch 173/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.049 - 0s 66us/sample - loss: 0.0679\n",
      "Epoch 174/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.022 - 0s 66us/sample - loss: 0.0646\n",
      "Epoch 175/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.054 - 0s 73us/sample - loss: 0.0660\n",
      "Epoch 176/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.029 - 0s 66us/sample - loss: 0.0640\n",
      "Epoch 177/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.107 - 0s 81us/sample - loss: 0.0653\n",
      "Epoch 178/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.098 - 0s 73us/sample - loss: 0.0634\n",
      "Epoch 179/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.046 - 0s 66us/sample - loss: 0.0623\n",
      "Epoch 180/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.020 - 0s 73us/sample - loss: 0.0610\n",
      "Epoch 181/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.058 - 0s 73us/sample - loss: 0.0592\n",
      "Epoch 182/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.084 - 0s 66us/sample - loss: 0.0605\n",
      "Epoch 183/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.052 - 0s 73us/sample - loss: 0.0593\n",
      "Epoch 184/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.084 - 0s 66us/sample - loss: 0.0598\n",
      "Epoch 185/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.066 - 0s 66us/sample - loss: 0.0594\n",
      "Epoch 186/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.095 - 0s 73us/sample - loss: 0.0569\n",
      "Epoch 187/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.082 - 0s 66us/sample - loss: 0.0563\n",
      "Epoch 188/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.049 - 0s 66us/sample - loss: 0.0565\n",
      "Epoch 189/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.051 - 0s 66us/sample - loss: 0.0547\n",
      "Epoch 190/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.022 - 0s 66us/sample - loss: 0.0562\n",
      "Epoch 191/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.051 - 0s 103us/sample - loss: 0.0554\n",
      "Epoch 192/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.079 - 0s 73us/sample - loss: 0.0546\n",
      "Epoch 193/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.061 - 0s 66us/sample - loss: 0.0545\n",
      "Epoch 194/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.043 - 0s 66us/sample - loss: 0.0536\n",
      "Epoch 195/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.018 - 0s 73us/sample - loss: 0.0542\n",
      "Epoch 196/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.039 - 0s 66us/sample - loss: 0.0534\n",
      "Epoch 197/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.145 - 0s 73us/sample - loss: 0.0541\n",
      "Epoch 198/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.059 - 0s 66us/sample - loss: 0.0522\n",
      "Epoch 199/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.017 - 0s 66us/sample - loss: 0.0516\n",
      "Epoch 200/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.021 - 0s 66us/sample - loss: 0.0506\n",
      "Epoch 201/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.019 - 0s 66us/sample - loss: 0.0501\n",
      "Epoch 202/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.015 - 0s 66us/sample - loss: 0.0501\n",
      "Epoch 203/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.037 - 0s 73us/sample - loss: 0.0494\n",
      "Epoch 204/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.035 - 0s 73us/sample - loss: 0.0472\n",
      "Epoch 205/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.035 - 0s 66us/sample - loss: 0.0483\n",
      "Epoch 206/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.054 - 0s 73us/sample - loss: 0.0477\n",
      "Epoch 207/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.040 - 0s 66us/sample - loss: 0.0484\n",
      "Epoch 208/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.036 - 0s 66us/sample - loss: 0.0495\n",
      "Epoch 209/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.060 - 0s 95us/sample - loss: 0.0490\n",
      "Epoch 210/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.071 - 0s 110us/sample - loss: 0.0485\n",
      "Epoch 211/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.013 - 0s 139us/sample - loss: 0.0461\n",
      "Epoch 212/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.027 - 0s 154us/sample - loss: 0.0461\n",
      "Epoch 213/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.010 - 0s 132us/sample - loss: 0.0452\n",
      "Epoch 214/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.044 - 0s 125us/sample - loss: 0.0442\n",
      "Epoch 215/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.048 - 0s 205us/sample - loss: 0.0444\n",
      "Epoch 216/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.042 - 0s 95us/sample - loss: 0.0455\n",
      "Epoch 217/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.091 - 0s 73us/sample - loss: 0.0470\n",
      "Epoch 218/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.016 - 0s 66us/sample - loss: 0.0463\n",
      "Epoch 219/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.019 - 0s 66us/sample - loss: 0.0446\n",
      "Epoch 220/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.039 - 0s 66us/sample - loss: 0.0423\n",
      "Epoch 221/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.018 - 0s 66us/sample - loss: 0.0415\n",
      "Epoch 222/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.043 - 0s 73us/sample - loss: 0.0429\n",
      "Epoch 223/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.035 - 0s 73us/sample - loss: 0.0436\n",
      "Epoch 224/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.137 - 0s 73us/sample - loss: 0.0457\n",
      "Epoch 225/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.031 - 0s 132us/sample - loss: 0.0405\n",
      "Epoch 226/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.019 - 0s 147us/sample - loss: 0.0423\n",
      "Epoch 227/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.073 - 0s 161us/sample - loss: 0.0387\n",
      "Epoch 228/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.044 - 0s 95us/sample - loss: 0.0415\n",
      "Epoch 229/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.026 - 0s 147us/sample - loss: 0.0379\n",
      "Epoch 230/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.063 - 0s 95us/sample - loss: 0.0406\n",
      "Epoch 231/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.027 - 0s 125us/sample - loss: 0.0400\n",
      "Epoch 232/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.023 - 0s 158us/sample - loss: 0.0383\n",
      "Epoch 233/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.092 - ETA: 0s - loss: 0.041 - 0s 214us/sample - loss: 0.0401\n",
      "Epoch 234/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.084 - 0s 138us/sample - loss: 0.0386\n",
      "Epoch 235/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.041 - 0s 150us/sample - loss: 0.0374\n",
      "Epoch 236/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.046 - 0s 129us/sample - loss: 0.0369\n",
      "Epoch 237/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.046 - 0s 138us/sample - loss: 0.0387\n",
      "Epoch 238/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.051 - 0s 141us/sample - loss: 0.0374\n",
      "Epoch 239/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.031 - 0s 144us/sample - loss: 0.0386\n",
      "Epoch 240/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.033 - 0s 123us/sample - loss: 0.0379\n",
      "Epoch 241/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.075 - 0s 126us/sample - loss: 0.0379\n",
      "Epoch 242/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.012 - 0s 120us/sample - loss: 0.0373\n",
      "Epoch 243/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.008 - 0s 76us/sample - loss: 0.0373\n",
      "Epoch 244/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - ETA: 0s - loss: 0.064 - 0s 67us/sample - loss: 0.0394\n",
      "Epoch 245/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.033 - 0s 67us/sample - loss: 0.0362\n",
      "Epoch 246/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.032 - 0s 67us/sample - loss: 0.0367\n",
      "Epoch 247/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.030 - 0s 67us/sample - loss: 0.0359\n",
      "Epoch 248/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.087 - 0s 70us/sample - loss: 0.0366\n",
      "Epoch 249/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.013 - 0s 70us/sample - loss: 0.0362\n",
      "Epoch 250/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.032 - 0s 132us/sample - loss: 0.0353\n",
      "Epoch 251/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.012 - 0s 147us/sample - loss: 0.0356\n",
      "Epoch 252/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.037 - 0s 217us/sample - loss: 0.0352\n",
      "Epoch 253/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.045 - 0s 141us/sample - loss: 0.0359\n",
      "Epoch 254/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.034 - 0s 153us/sample - loss: 0.0332\n",
      "Epoch 255/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.007 - 0s 76us/sample - loss: 0.0359\n",
      "Epoch 256/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.022 - 0s 67us/sample - loss: 0.0339\n",
      "Epoch 257/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.054 - 0s 67us/sample - loss: 0.0348\n",
      "Epoch 258/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.035 - 0s 67us/sample - loss: 0.0350\n",
      "Epoch 259/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.012 - 0s 67us/sample - loss: 0.0337\n",
      "Epoch 260/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.028 - 0s 70us/sample - loss: 0.0358\n",
      "Epoch 261/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.056 - 0s 67us/sample - loss: 0.0351\n",
      "Epoch 262/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.007 - 0s 65us/sample - loss: 0.0335\n",
      "Epoch 263/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.009 - 0s 67us/sample - loss: 0.0355\n",
      "Epoch 264/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.008 - 0s 67us/sample - loss: 0.0343\n",
      "Epoch 265/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.046 - 0s 79us/sample - loss: 0.0338\n",
      "Epoch 266/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.012 - 0s 85us/sample - loss: 0.0333\n",
      "Epoch 267/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.060 - 0s 79us/sample - loss: 0.0338\n",
      "Epoch 268/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.010 - 0s 79us/sample - loss: 0.0333\n",
      "Epoch 269/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.030 - 0s 94us/sample - loss: 0.0328\n",
      "Epoch 270/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.014 - 0s 76us/sample - loss: 0.0325\n",
      "Epoch 271/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.036 - 0s 67us/sample - loss: 0.0338\n",
      "Epoch 272/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.011 - 0s 120us/sample - loss: 0.0327\n",
      "Epoch 273/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.023 - 0s 147us/sample - loss: 0.0324\n",
      "Epoch 274/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.031 - 0s 132us/sample - loss: 0.0349\n",
      "Epoch 275/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.012 - 0s 158us/sample - loss: 0.0320\n",
      "Epoch 276/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.038 - 0s 129us/sample - loss: 0.0325\n",
      "Epoch 277/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.026 - 0s 144us/sample - loss: 0.0332\n",
      "Epoch 278/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.045 - 0s 117us/sample - loss: 0.0328\n",
      "Epoch 279/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.004 - 0s 70us/sample - loss: 0.0332\n",
      "Epoch 280/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.054 - 0s 70us/sample - loss: 0.0320\n",
      "Epoch 281/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.009 - 0s 67us/sample - loss: 0.0332\n",
      "Epoch 282/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.144 - 0s 67us/sample - loss: 0.0345\n",
      "Epoch 283/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.118 - 0s 73us/sample - loss: 0.0331\n",
      "Epoch 284/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.027 - 0s 67us/sample - loss: 0.0317\n",
      "Epoch 285/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.055 - 0s 67us/sample - loss: 0.0315\n",
      "Epoch 286/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.022 - 0s 67us/sample - loss: 0.0322\n",
      "Epoch 287/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.047 - 0s 67us/sample - loss: 0.0303\n",
      "Epoch 288/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.035 - 0s 67us/sample - loss: 0.0306\n",
      "Epoch 289/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.005 - 0s 70us/sample - loss: 0.0313\n",
      "Epoch 290/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.071 - 0s 73us/sample - loss: 0.0307\n",
      "Epoch 291/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.009 - 0s 70us/sample - loss: 0.0301\n",
      "Epoch 292/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.027 - 0s 76us/sample - loss: 0.0308\n",
      "Epoch 293/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.006 - 0s 91us/sample - loss: 0.0310\n",
      "Epoch 294/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.041 - 0s 97us/sample - loss: 0.0318\n",
      "Epoch 295/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.002 - 0s 79us/sample - loss: 0.0306\n",
      "Epoch 296/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.008 - 0s 88us/sample - loss: 0.0307\n",
      "Epoch 297/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0312\n",
      "Epoch 298/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.074 - 0s 88us/sample - loss: 0.0314\n",
      "Epoch 299/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.032 - 0s 85us/sample - loss: 0.0303\n",
      "Epoch 300/300\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.032 - 0s 79us/sample - loss: 0.0298\n",
      "Start talking with the bot (type quit to stop)!\n",
      "You: who owns you\n",
      "Stems : ['who', 'own', 'you']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Highest intent proability: 0.80704564\n",
      "Intent of sentence :  working\n",
      "chatbot: I am not rule based\n",
      "You: who is your owner\n",
      "Stems : ['who', 'is', 'your', 'owner']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "Highest intent proability: 0.9992341\n",
      "Intent of sentence :  owner\n",
      "chatbot: I am outcome of graduate final project\n",
      "You: who created you\n",
      "Stems : ['who', 'creat', 'you']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Highest intent proability: 0.989427\n",
      "Intent of sentence :  owner\n",
      "chatbot: Preeti and Shabbir made me\n",
      "You: what all you can do\n",
      "Stems : ['what', 'all', 'you', 'can', 'do']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Highest intent proability: 0.9950472\n",
      "Intent of sentence :  working\n",
      "chatbot: I use neural network model\n",
      "You: what tasks you can do?\n",
      "Stems : ['what', 'task', 'you', 'can', 'do', '?']\n",
      "Vector Representation of input sentence is: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Highest intent proability: 0.50409377\n",
      "Intent of sentence :  Name\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chatbot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-18aebb2cafb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-18aebb2cafb1>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0mchatbot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChatbot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0mchatbot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-18aebb2cafb1>\u001b[0m in \u001b[0;36mchat\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mProbability\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m.7\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                 \u001b[0mChatbot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedback_save_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'***** I am not able to get that clearly best possible response I can give is *****'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-18aebb2cafb1>\u001b[0m in \u001b[0;36mfeedback_save_to_file\u001b[1;34m(sentence, PredictedTag)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mfeedbackDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PredictedTag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPredictedTag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchatbot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedbackFileName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m                 \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeedbackDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chatbot' is not defined"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "class Chatbot:\n",
    "\n",
    "    showWorking = False\n",
    "    train_again=False\n",
    "    vocabulary=[]\n",
    "    labels=[]\n",
    "    responseDictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "    labelFileName= 'LabelList.json'\n",
    "    vocabularyFileName = 'VocabularyList.json'\n",
    "    responseFileName = 'responseDictionary.json'\n",
    "    dataSetFileName = 'CommonIntentions.json'\n",
    "    feedbackFileName = 'feedback.json'\n",
    "    modelFileName = 'chatBotContext.h5'\n",
    "    \n",
    "    def __init__(self,train_again=False):\n",
    "        Chatbot.train_again = train_again\n",
    "\n",
    "    def get_model(self):\n",
    "\n",
    "        if not Chatbot.train_again:\n",
    "            with open (Chatbot.labelFileName) as fl:\n",
    "                Chatbot.labels = json.load(fl)\n",
    "            \n",
    "            with open (Chatbot.vocabularyFileName) as fv:\n",
    "                Chatbot.vocabulary = json.load(fv)\n",
    "                \n",
    "            with open (responseFileName) as fr:\n",
    "                Chatbot.responseDictionary = json.load(fr)\n",
    "                \n",
    "            model= tf.keras.models.load_model(Chatbot.modelFileName)\n",
    "        else:\n",
    "            with open(Chatbot.dataSetFileName) as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            docs_x = []\n",
    "            docs_y = []\n",
    "\n",
    "            for intent in data['intents']:\n",
    "                \n",
    "                Chatbot.responseDictionary[intent['tag']] = []\n",
    "                responseList=[]\n",
    "                for response in intent['responses']:\n",
    "                    responseList.append(response)\n",
    "                    \n",
    "                Chatbot.responseDictionary[intent['tag']] = responseList\n",
    "                \n",
    "                for pattern in intent['patterns']:\n",
    "                    wrds = remove_stop_words_get_lemmas(pattern)\n",
    "                    if len(wrds)>0:\n",
    "                        Chatbot.vocabulary.extend(wrds)\n",
    "                        docs_x.append(wrds)\n",
    "                        docs_y.append(intent['tag'])               \n",
    "\n",
    "                if intent['tag'] not in Chatbot.labels:\n",
    "                    Chatbot.labels.append(intent['tag'])\n",
    "                    \n",
    "            with open(Chatbot.responseFileName, 'w') as fp:\n",
    "                json.dump(Chatbot.responseDictionary, fp)\n",
    "                \n",
    "            Chatbot.vocabulary = sorted(list(set(Chatbot.vocabulary)))\n",
    "            Chatbot.labels = sorted(Chatbot.labels)\n",
    "            training = []\n",
    "            output = []\n",
    "            out_empty = [0 for _ in range(len(Chatbot.labels))]\n",
    "\n",
    "            for x, doc in enumerate(docs_x):\n",
    "                bag = []\n",
    "\n",
    "                for w in Chatbot.vocabulary:\n",
    "                    if w in doc:\n",
    "                        bag.append(1)\n",
    "                    else:\n",
    "                        bag.append(0)\n",
    "\n",
    "                output_row = out_empty[:]\n",
    "                output_row[Chatbot.labels.index(docs_y[x])] = 1\n",
    "\n",
    "                training.append(bag)\n",
    "                output.append(output_row)\n",
    "\n",
    "            training = numpy.array(training)\n",
    "            output = numpy.array(output)\n",
    "\n",
    "\n",
    "            model = tf.keras.Sequential()\n",
    "            for layer in range(3):\n",
    "            # Adds a densely-connected layer with specified units to the model:\n",
    "                model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "\n",
    "        # Add a sigmoid layer with 1 output units:\n",
    "            model.add(tf.keras.layers.Dense(len(output[0]), activation='softmax'))\n",
    "            model.compile(optimizer='adam',loss='categorical_crossentropy',metric=['accuracy'])\n",
    "            model.fit(training, output, epochs=300)\n",
    "            model.save(Chatbot.modelFileName)\n",
    "            \n",
    "            with open(Chatbot.labelFileName, 'w') as fl:\n",
    "                json.dump(Chatbot.labels, fl)\n",
    "            with open(Chatbot.vocabularyFileName, 'w') as fv:\n",
    "                json.dump(Chatbot.vocabulary, fv)\n",
    "        return model\n",
    "\n",
    "    def get_yes_list(self):\n",
    "        return ['yes','sure','ok','okay','for sure','sure','totally','yep','ya','yeah','yup','certainly',\n",
    "                         'definitely','of course','gladly','absolutely','indeed']\n",
    "    @staticmethod\n",
    "    def feedback_save_to_file(sentence,PredictedTag):\n",
    "        feedbackDict = {}\n",
    "        feedbackDict['input'] = sentence\n",
    "        feedbackDict['PredictedTag']=PredictedTag\n",
    "        \n",
    "        with open(chatbot.feedbackFileName, 'a+') as fv:\n",
    "                json.dump(feedbackDict, fv)\n",
    "        \n",
    "    \n",
    "    def chat(self):\n",
    "        \n",
    "        print(\"Do you want to see internal processing steps:\")\n",
    "        choice = input(\"Enter you choice : \")\n",
    "        if choice.lower() in self.get_yes_list():\n",
    "            Chatbot.showWorking=True\n",
    "        model = self.get_model()\n",
    "        print(\"Start talking with the bot (type quit to stop)!\")\n",
    "        while True:\n",
    "            inp = input(\"You: \")\n",
    "            if inp.lower() == \"quit\":\n",
    "                break\n",
    "            results = model.predict(bag_of_words(inp))\n",
    "            results_index = numpy.argmax(results)\n",
    "            tag = Chatbot.labels[results_index]\n",
    "            Probability = results[0][results_index]\n",
    "            if Chatbot.showWorking:\n",
    "                print(\"Highest intent proability:\",Probability)\n",
    "                print(\"Intent of sentence : \",tag)\n",
    "            responses = Chatbot.responseDictionary[tag]\n",
    "            \n",
    "            if Probability < .7:\n",
    "                Chatbot.feedback_save_to_file(inp,tag)\n",
    "                print('***** I am not able to get that clearly best possible response I can give is *****')\n",
    "                \n",
    "            print(\"chatbot:\",random.choice(responses))\n",
    "            \n",
    "def remove_stop_words_get_lemmas(sentence):\n",
    "        s_words = nltk.word_tokenize(sentence)\n",
    "        s_words = [Chatbot.stemmer.stem(word.lower()) for word in s_words]\n",
    "        if Chatbot.showWorking:\n",
    "            print('Stems :',s_words)\n",
    "        return s_words\n",
    "\n",
    "def bag_of_words(sentence):\n",
    "    bags = []\n",
    "    \n",
    "    #creating hot vector\n",
    "    bag = [0 for _ in range(len(Chatbot.vocabulary))] \n",
    "    s_words = remove_stop_words_get_lemmas(sentence)\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(Chatbot.vocabulary):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "                \n",
    "    if Chatbot.showWorking:\n",
    "        print('Vector Representation of input sentence is:',bag)\n",
    "            \n",
    "    bags.append(bag)\n",
    "    bags = numpy.array(bags)\n",
    "    return bags\n",
    "        \n",
    "def main():\n",
    "    chatbot = Chatbot(False)\n",
    "    chatbot.chat()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from importlib import reload \n",
    "reload(spacy)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "while (True):\n",
    "\n",
    "    doc = input(\"Enter :\")\n",
    "\n",
    "    if doc=='quit':\n",
    "        break\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    \n",
    "    doc = spacy_nlp(doc)\n",
    "    customize_stop_words = [\n",
    "        '.', '?','!'\n",
    "    ]\n",
    "    \n",
    "    for w in customize_stop_words:\n",
    "        spacy_nlp.vocab[w].is_stop = True\n",
    "\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "print(spacy.lang.en.stop_words.STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bags = []\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    #print('tokenizer:',s_words)\n",
    "    \n",
    "    print('lemmarizer:',get_lemmas(s))\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    bags.append(bag)\n",
    "    bags = numpy.array(bags)\n",
    "    #return numpy.array(bag)\n",
    "    return bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "    \n",
    "with open ('responseDictionary', 'rb') as fp:\n",
    "    itemlist2 = pickle.load(fp)\n",
    "    \n",
    "print(itemlist2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "#from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "sentence = 'what are you'\n",
    "s_words = nltk.word_tokenize(sentence)\n",
    "s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "print(s_words)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'asds': 'Geeks', 'b': 'For', 'c': 'geeks'} \n",
    "\n",
    "def getList(dict): \n",
    "      \n",
    "    return [*dict] \n",
    "\n",
    "print(dict['dhjjh']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
